<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.0.0"/>
    <title>deeplearningnumpy.models API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../deeplearningnumpy.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;deeplearningnumpy</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#NeuralNetwork">NeuralNetwork</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#NeuralNetwork.__init__">NeuralNetwork</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.forward">forward</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.getOutputs">getOutputs</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.saveWeights">saveWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.loadWeights">loadWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.updateWeights">updateWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.train">train</a>
                        </li>
                        <li>
                                <a class="function" href="#NeuralNetwork.getEstimatedGradients">getEstimatedGradients</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../deeplearningnumpy.html">deeplearningnumpy</a><wbr>.models    </h1>

                
                        <input id="mod-models-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-models-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="kn">import</span> <span class="nn">os</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="kn">from</span> <span class="nn">deeplearningnumpy.cost_functions</span> <span class="kn">import</span> <span class="n">CategoricalCrossEntropy</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="kn">from</span> <span class="nn">deeplearningnumpy.activations</span> <span class="kn">import</span> <span class="n">ActivationSoftmax</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="kn">import</span> <span class="nn">pickle</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents a neural network, which may be fully connected or convolutional.</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="sd">    </span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="sd">    Attributes</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="sd">    ----------</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="sd">    name : str</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="sd">        A string identifier for the neural network e.g. for naming weights files.</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="sd">    layers : array_like</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="sd">        A list of layers that comprise the neural network. Inputs are propagated through this set of layers</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="sd">        sequentially from start to finish when the `forward` method is called.</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">layers</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Performs a forward pass of the network.</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="sd">        </span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="sd">        Parameters</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">        ----------</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">            The inputs to the network, which should match the dimensions required for</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="sd">            the input to the `forward` method of the first layer in the network.</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>        <span class="c1"># Initialise inputs to starting values</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="n">nextInputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">nextInputs</span><span class="p">)</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>            <span class="c1"># Initialise inputs of next layer as outputs from current layer</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>            <span class="n">nextInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>    <span class="k">def</span> <span class="nf">getOutputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the outputs from the last layer of the network produced by the</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="sd">        most recent call to `forward`.</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="sd">        </span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="sd">        Returns</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="sd">        -------</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">        numpy.ndarray</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">            The outputs of the last layer, whose dimensions match those of the</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">            output of the `forward` method in the last layer of the network.</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>    <span class="k">def</span> <span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves the current weights for the network in a pickle file.</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        The file is named as ``{network_name}.pkl``, and saved in the</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        ``deeplearningnumpy/data/`` directory. </span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>        <span class="n">fileName</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;deeplearningnumpy/data/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a>        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fileHandle</span><span class="p">:</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a>            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">,</span> <span class="n">fileHandle</span><span class="p">)</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>    <span class="k">def</span> <span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the weights for this network from the appropriate pickle file (if it exists).</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">        The file that is read is ``deeplearningnumpy/data/{network_name}.pkl``.</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">        Raises</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="sd">        ------</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="sd">        FileNotFoundError</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a><span class="sd">            If the file for the network does not exist.</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>        <span class="n">fileName</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;deeplearningnumpy/data/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">fileName</span><span class="p">):</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights for network &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&#39; could not be loaded - have you trained your model?&quot;</span><span class="p">)</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fileHandle</span><span class="p">:</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileHandle</span><span class="p">)</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>    <span class="k">def</span> <span class="nf">updateWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">expectedOutputs</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform weight updates for all layers in the network.</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="sd">        </span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="sd">        A forward pass is made prior to updating weights to ensure that the layer outputs</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="sd">        are those produced by inputting `inputs`. The weights are then updated to</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="sd">        bring the outputs closer to `expectedOutputs`, according to `costFunction` and </span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a><span class="sd">        controlled by the `learningRate`.</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="sd">        Parameters</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a><span class="sd">        ----------</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a><span class="sd">            The inputs to the network, which should match the dimensions required for</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="sd">            the input to the `forward` method of the first layer in the network.</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="sd">        expectedOutputs : numpy.ndarray</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a><span class="sd">            The desired values for the network&#39;s outputs. This parameter should have the same dimensions as that</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a><span class="sd">            which would be produced by the network&#39;s `forward` method.</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="sd">            A cost function whose derivative is calculated with respect to the outputs of the network</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">            (i.e. the outputs of the last layer).</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a><span class="sd">        learningRate : float</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="sd">            Controls the amount by which the weights and biases of each layer are changed on each step. </span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="sd">            Both the weights and biases of each layer are updated at the same rate.</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a><span class="sd">        Notes</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a><span class="sd">        -----</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>        <span class="c1">#Iterate over each layer, starting with the output layer</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>            <span class="n">currentLayer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>            <span class="c1">#Get the delta values for this layer</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getOutputDeltas</span><span class="p">(</span><span class="n">expectedOutputs</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">)</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getDeltas</span><span class="p">(</span><span class="n">currentLayer</span><span class="o">.</span><span class="n">outputDeltas</span><span class="p">)</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getDeltas</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">inputDeltas</span><span class="p">)</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>            <span class="c1">#Get error gradients</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">currentLayer</span><span class="o">.</span><span class="n">getErrorGradients</span><span class="p">())</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="c1">#Update all network weights and biases</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>            <span class="n">currentLayer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>            <span class="c1">#Reshape error gradients so that they can be added to the weights</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>            <span class="n">currentLayer</span><span class="o">.</span><span class="n">updateWeights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">batchSize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">checkGradients</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">testImages</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">testLabels</span> <span class="o">=</span> <span class="p">[]):</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Trains the network using stochastic gradient descent.</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="sd">        </span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">        Parameters</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a><span class="sd">        ----------</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="sd">        X : numpy.ndarray</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="sd">            The training inputs. The first dimension equals the total number of input samples (not batches).</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="sd">        Y : numpy.ndarray</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a><span class="sd">            The desired training outputs. The first dimension equals the total number of input samples (not batches).</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a><span class="sd">            A cost function to evaluate the performance of the neural network.</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a><span class="sd">        batchSize : int</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a><span class="sd">            The number of input samples to include in each batch.</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a><span class="sd">        epochs : int, optional</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a><span class="sd">            The number of times to iterate through the full training set. Defaults to 10.</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a><span class="sd">        learningRate : float, optional</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a><span class="sd">            Controls the rate of learning. A small value of `learningRate` will lead to slow training,</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a><span class="sd">            but a large value will cause training instability. Defaults to 0.1.</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a><span class="sd">        checkGradients : bool, optional</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a><span class="sd">            Whether to perform gradient checking. This was used for verifying the gradients computed through</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a><span class="sd">            backpropagation were correct, and is therefore unlikely to be useful to a user. Defaults to False.</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a><span class="sd">        testImages : np.ndarray, optional</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a><span class="sd">            Test data to evaluate the accuracy of the model on a test set during the training process.</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a><span class="sd">            Defaults to [].</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="sd">        testLabels : np.ndarray, optional</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a><span class="sd">            Corresponding labels for `testImages` to evaluate the accuracy of the model on a test set during</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a><span class="sd">            the training process. Defaults to [].</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="sd">        Notes</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a><span class="sd">        -----</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>        <span class="c1">#If the cost function is CCE, the final activation must be softmax</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">costFunction</span><span class="p">,</span> <span class="n">CategoricalCrossEntropy</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activationFunction</span><span class="p">,</span> <span class="n">ActivationSoftmax</span><span class="p">):</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The Softmax function can only be used with CCE loss&quot;</span><span class="p">)</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>        
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>        <span class="c1">#Get number of training examples</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>        <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">batchSize</span><span class="p">:</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>            <span class="n">batchSize</span> <span class="o">=</span> <span class="n">m</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>        <span class="n">startIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>            <span class="c1">#Iterate over all batches in the training set</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span> <span class="o">//</span> <span class="n">batchSize</span><span class="p">):</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>                <span class="k">try</span><span class="p">:</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>                    <span class="c1">#Get the current batch data</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>                    <span class="n">currentBatch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">startIndex</span><span class="p">:</span><span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">]</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>                    <span class="n">currentLabels</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">startIndex</span><span class="p">:</span><span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">]</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>                    <span class="c1">#Perform a forward pass using the input data</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">)</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">updateWeights</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">,</span> <span class="n">currentLabels</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>                    <span class="c1">#Print cost for mini batch</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cost: </span><span class="si">{</span><span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">currentLabels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>                    <span class="c1">#Numerically estimate the gradients to verify backprop implementation</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>                    <span class="k">if</span> <span class="n">checkGradients</span><span class="p">:</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error gradients (by backprop): </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Numerically estimated gradients: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">getEstimatedGradients</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">,</span><span class="w"> </span><span class="n">currentLabels</span><span class="p">,</span><span class="w"> </span><span class="n">costFunction</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>                    <span class="c1"># Evaluate accuracy on test set</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a><span class="w">                    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a><span class="sd">                    if testImages != [] and testLabels != []:</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a><span class="sd">                        self.forward(testImages)</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a><span class="sd">                        testOutputs = np.argmax(self.getOutputs(), -1)</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a><span class="sd">                        matchingCount = 0</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a><span class="sd">                        for i in range(testOutputs.size):</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a><span class="sd">                            if testOutputs[i] == np.argmax(testLabels[i], -1):</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a><span class="sd">                                matchingCount += 1</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a><span class="sd">                        print(&quot;Accuracy on test set: {}%&quot;.format((matchingCount / testOutputs.size) * 100))</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a><span class="sd">                    &#39;&#39;&#39;</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>                    <span class="n">startIndex</span> <span class="o">+=</span> <span class="n">batchSize</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>                    <span class="k">if</span> <span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span> <span class="o">&gt;</span> <span class="n">m</span><span class="p">:</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>                        <span class="n">startIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>                    
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>                <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>                    <span class="k">break</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>    <span class="k">def</span> <span class="nf">getEstimatedGradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">):</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Numerically estimates the partial derivatives of the cost function w.r.t each weight.</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a><span class="sd">        </span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a><span class="sd">        This is used in gradient checking as an alternative way of computing gradients other</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a><span class="sd">        than backpropagation, but is unlikely to be useful to a user.</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a><span class="sd">        </span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a><span class="sd">        Parameters</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a><span class="sd">        ----------</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a><span class="sd">            The inputs used for evaluating cost, and hence estimating derivatives.</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a><span class="sd">        yReal : numpy.ndarray</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a><span class="sd">            The correct outputs for the given inputs, used for evaluating cost.</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a><span class="sd">            The cost function used for taking derivatives.</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a><span class="sd">        Returns</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a><span class="sd">        -------</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a><span class="sd">        array-like</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a><span class="sd">            The estimated gradients with respect to the weights and biases of each layer.</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a><span class="sd">        </span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a><span class="sd">        Notes</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a><span class="sd">        -----</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>        <span class="c1">#Assemble a list of computed gradient matrices</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>        <span class="n">estimatedGradients</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>        <span class="c1"># Iterate over each bias parameter in each layer</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>            <span class="n">flattenedBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>            <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">flattenedBiases</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flattenedBiases</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>                <span class="c1">#A small value so that the gradient can be calculated between 2 points</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>                <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>                <span class="c1">#Copy the biases matrix so that it can be replaced once gradient checking completes</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>                <span class="n">biasesCopy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>                <span class="n">flattenedBiases</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">EPSILON</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>                <span class="n">gradPlus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>                <span class="n">flattenedBiases</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>                <span class="n">gradMinus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>                <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradPlus</span> <span class="o">-</span> <span class="n">gradMinus</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span><span class="p">)</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>                <span class="c1">#Re-instate original layer biases</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">biasesCopy</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>                <span class="n">flattenedBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>        <span class="c1">#Iterate over each weight parameter in each layer</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>            <span class="n">flattenedWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>            <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">flattenedWeights</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flattenedWeights</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>                <span class="c1">#A small value so that the gradient can be calculated between 2 points</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>                <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>                <span class="c1">#Copy the weights matrix so that it can be replaced once gradient checking completes</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>                <span class="n">weightsCopy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>                <span class="n">flattenedWeights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">EPSILON</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>                <span class="n">gradPlus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>                <span class="n">flattenedWeights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>                <span class="n">gradMinus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>                <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradPlus</span> <span class="o">-</span> <span class="n">gradMinus</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span><span class="p">)</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>                <span class="c1">#Re-instate original layer weights</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weightsCopy</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>                <span class="n">flattenedWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>        <span class="k">return</span> <span class="n">estimatedGradients</span>
</span></pre></div>


            </section>
                <section id="NeuralNetwork">
                            <input id="NeuralNetwork-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">NeuralNetwork</span>:

                <label class="view-source-button" for="NeuralNetwork-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork-8"><a href="#NeuralNetwork-8"><span class="linenos">  8</span></a><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
</span><span id="NeuralNetwork-9"><a href="#NeuralNetwork-9"><span class="linenos">  9</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents a neural network, which may be fully connected or convolutional.</span>
</span><span id="NeuralNetwork-10"><a href="#NeuralNetwork-10"><span class="linenos"> 10</span></a><span class="sd">    </span>
</span><span id="NeuralNetwork-11"><a href="#NeuralNetwork-11"><span class="linenos"> 11</span></a><span class="sd">    Attributes</span>
</span><span id="NeuralNetwork-12"><a href="#NeuralNetwork-12"><span class="linenos"> 12</span></a><span class="sd">    ----------</span>
</span><span id="NeuralNetwork-13"><a href="#NeuralNetwork-13"><span class="linenos"> 13</span></a><span class="sd">    name : str</span>
</span><span id="NeuralNetwork-14"><a href="#NeuralNetwork-14"><span class="linenos"> 14</span></a><span class="sd">        A string identifier for the neural network e.g. for naming weights files.</span>
</span><span id="NeuralNetwork-15"><a href="#NeuralNetwork-15"><span class="linenos"> 15</span></a><span class="sd">    layers : array_like</span>
</span><span id="NeuralNetwork-16"><a href="#NeuralNetwork-16"><span class="linenos"> 16</span></a><span class="sd">        A list of layers that comprise the neural network. Inputs are propagated through this set of layers</span>
</span><span id="NeuralNetwork-17"><a href="#NeuralNetwork-17"><span class="linenos"> 17</span></a><span class="sd">        sequentially from start to finish when the `forward` method is called.</span>
</span><span id="NeuralNetwork-18"><a href="#NeuralNetwork-18"><span class="linenos"> 18</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-19"><a href="#NeuralNetwork-19"><span class="linenos"> 19</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
</span><span id="NeuralNetwork-20"><a href="#NeuralNetwork-20"><span class="linenos"> 20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
</span><span id="NeuralNetwork-21"><a href="#NeuralNetwork-21"><span class="linenos"> 21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">layers</span>
</span><span id="NeuralNetwork-22"><a href="#NeuralNetwork-22"><span class="linenos"> 22</span></a>
</span><span id="NeuralNetwork-23"><a href="#NeuralNetwork-23"><span class="linenos"> 23</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="NeuralNetwork-24"><a href="#NeuralNetwork-24"><span class="linenos"> 24</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Performs a forward pass of the network.</span>
</span><span id="NeuralNetwork-25"><a href="#NeuralNetwork-25"><span class="linenos"> 25</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-26"><a href="#NeuralNetwork-26"><span class="linenos"> 26</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork-27"><a href="#NeuralNetwork-27"><span class="linenos"> 27</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork-28"><a href="#NeuralNetwork-28"><span class="linenos"> 28</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="NeuralNetwork-29"><a href="#NeuralNetwork-29"><span class="linenos"> 29</span></a><span class="sd">            The inputs to the network, which should match the dimensions required for</span>
</span><span id="NeuralNetwork-30"><a href="#NeuralNetwork-30"><span class="linenos"> 30</span></a><span class="sd">            the input to the `forward` method of the first layer in the network.</span>
</span><span id="NeuralNetwork-31"><a href="#NeuralNetwork-31"><span class="linenos"> 31</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-32"><a href="#NeuralNetwork-32"><span class="linenos"> 32</span></a>        <span class="c1"># Initialise inputs to starting values</span>
</span><span id="NeuralNetwork-33"><a href="#NeuralNetwork-33"><span class="linenos"> 33</span></a>        <span class="n">nextInputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-34"><a href="#NeuralNetwork-34"><span class="linenos"> 34</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="NeuralNetwork-35"><a href="#NeuralNetwork-35"><span class="linenos"> 35</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">nextInputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-36"><a href="#NeuralNetwork-36"><span class="linenos"> 36</span></a>            <span class="c1"># Initialise inputs of next layer as outputs from current layer</span>
</span><span id="NeuralNetwork-37"><a href="#NeuralNetwork-37"><span class="linenos"> 37</span></a>            <span class="n">nextInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span><span id="NeuralNetwork-38"><a href="#NeuralNetwork-38"><span class="linenos"> 38</span></a>
</span><span id="NeuralNetwork-39"><a href="#NeuralNetwork-39"><span class="linenos"> 39</span></a>    <span class="k">def</span> <span class="nf">getOutputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="NeuralNetwork-40"><a href="#NeuralNetwork-40"><span class="linenos"> 40</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the outputs from the last layer of the network produced by the</span>
</span><span id="NeuralNetwork-41"><a href="#NeuralNetwork-41"><span class="linenos"> 41</span></a><span class="sd">        most recent call to `forward`.</span>
</span><span id="NeuralNetwork-42"><a href="#NeuralNetwork-42"><span class="linenos"> 42</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-43"><a href="#NeuralNetwork-43"><span class="linenos"> 43</span></a><span class="sd">        Returns</span>
</span><span id="NeuralNetwork-44"><a href="#NeuralNetwork-44"><span class="linenos"> 44</span></a><span class="sd">        -------</span>
</span><span id="NeuralNetwork-45"><a href="#NeuralNetwork-45"><span class="linenos"> 45</span></a><span class="sd">        numpy.ndarray</span>
</span><span id="NeuralNetwork-46"><a href="#NeuralNetwork-46"><span class="linenos"> 46</span></a><span class="sd">            The outputs of the last layer, whose dimensions match those of the</span>
</span><span id="NeuralNetwork-47"><a href="#NeuralNetwork-47"><span class="linenos"> 47</span></a><span class="sd">            output of the `forward` method in the last layer of the network.</span>
</span><span id="NeuralNetwork-48"><a href="#NeuralNetwork-48"><span class="linenos"> 48</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-49"><a href="#NeuralNetwork-49"><span class="linenos"> 49</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span><span id="NeuralNetwork-50"><a href="#NeuralNetwork-50"><span class="linenos"> 50</span></a>
</span><span id="NeuralNetwork-51"><a href="#NeuralNetwork-51"><span class="linenos"> 51</span></a>    <span class="k">def</span> <span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="NeuralNetwork-52"><a href="#NeuralNetwork-52"><span class="linenos"> 52</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves the current weights for the network in a pickle file.</span>
</span><span id="NeuralNetwork-53"><a href="#NeuralNetwork-53"><span class="linenos"> 53</span></a>
</span><span id="NeuralNetwork-54"><a href="#NeuralNetwork-54"><span class="linenos"> 54</span></a><span class="sd">        The file is named as ``{network_name}.pkl``, and saved in the</span>
</span><span id="NeuralNetwork-55"><a href="#NeuralNetwork-55"><span class="linenos"> 55</span></a><span class="sd">        ``deeplearningnumpy/data/`` directory. </span>
</span><span id="NeuralNetwork-56"><a href="#NeuralNetwork-56"><span class="linenos"> 56</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-57"><a href="#NeuralNetwork-57"><span class="linenos"> 57</span></a>        <span class="n">fileName</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;deeplearningnumpy/data/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
</span><span id="NeuralNetwork-58"><a href="#NeuralNetwork-58"><span class="linenos"> 58</span></a>        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fileHandle</span><span class="p">:</span>
</span><span id="NeuralNetwork-59"><a href="#NeuralNetwork-59"><span class="linenos"> 59</span></a>            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">,</span> <span class="n">fileHandle</span><span class="p">)</span>
</span><span id="NeuralNetwork-60"><a href="#NeuralNetwork-60"><span class="linenos"> 60</span></a>
</span><span id="NeuralNetwork-61"><a href="#NeuralNetwork-61"><span class="linenos"> 61</span></a>    <span class="k">def</span> <span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="NeuralNetwork-62"><a href="#NeuralNetwork-62"><span class="linenos"> 62</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the weights for this network from the appropriate pickle file (if it exists).</span>
</span><span id="NeuralNetwork-63"><a href="#NeuralNetwork-63"><span class="linenos"> 63</span></a>
</span><span id="NeuralNetwork-64"><a href="#NeuralNetwork-64"><span class="linenos"> 64</span></a><span class="sd">        The file that is read is ``deeplearningnumpy/data/{network_name}.pkl``.</span>
</span><span id="NeuralNetwork-65"><a href="#NeuralNetwork-65"><span class="linenos"> 65</span></a>
</span><span id="NeuralNetwork-66"><a href="#NeuralNetwork-66"><span class="linenos"> 66</span></a><span class="sd">        Raises</span>
</span><span id="NeuralNetwork-67"><a href="#NeuralNetwork-67"><span class="linenos"> 67</span></a><span class="sd">        ------</span>
</span><span id="NeuralNetwork-68"><a href="#NeuralNetwork-68"><span class="linenos"> 68</span></a><span class="sd">        FileNotFoundError</span>
</span><span id="NeuralNetwork-69"><a href="#NeuralNetwork-69"><span class="linenos"> 69</span></a><span class="sd">            If the file for the network does not exist.</span>
</span><span id="NeuralNetwork-70"><a href="#NeuralNetwork-70"><span class="linenos"> 70</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-71"><a href="#NeuralNetwork-71"><span class="linenos"> 71</span></a>        <span class="n">fileName</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;deeplearningnumpy/data/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
</span><span id="NeuralNetwork-72"><a href="#NeuralNetwork-72"><span class="linenos"> 72</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">fileName</span><span class="p">):</span>
</span><span id="NeuralNetwork-73"><a href="#NeuralNetwork-73"><span class="linenos"> 73</span></a>            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights for network &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&#39; could not be loaded - have you trained your model?&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork-74"><a href="#NeuralNetwork-74"><span class="linenos"> 74</span></a>
</span><span id="NeuralNetwork-75"><a href="#NeuralNetwork-75"><span class="linenos"> 75</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="NeuralNetwork-76"><a href="#NeuralNetwork-76"><span class="linenos"> 76</span></a>            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fileHandle</span><span class="p">:</span>
</span><span id="NeuralNetwork-77"><a href="#NeuralNetwork-77"><span class="linenos"> 77</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileHandle</span><span class="p">)</span>
</span><span id="NeuralNetwork-78"><a href="#NeuralNetwork-78"><span class="linenos"> 78</span></a>
</span><span id="NeuralNetwork-79"><a href="#NeuralNetwork-79"><span class="linenos"> 79</span></a>    <span class="k">def</span> <span class="nf">updateWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">expectedOutputs</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="NeuralNetwork-80"><a href="#NeuralNetwork-80"><span class="linenos"> 80</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform weight updates for all layers in the network.</span>
</span><span id="NeuralNetwork-81"><a href="#NeuralNetwork-81"><span class="linenos"> 81</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-82"><a href="#NeuralNetwork-82"><span class="linenos"> 82</span></a><span class="sd">        A forward pass is made prior to updating weights to ensure that the layer outputs</span>
</span><span id="NeuralNetwork-83"><a href="#NeuralNetwork-83"><span class="linenos"> 83</span></a><span class="sd">        are those produced by inputting `inputs`. The weights are then updated to</span>
</span><span id="NeuralNetwork-84"><a href="#NeuralNetwork-84"><span class="linenos"> 84</span></a><span class="sd">        bring the outputs closer to `expectedOutputs`, according to `costFunction` and </span>
</span><span id="NeuralNetwork-85"><a href="#NeuralNetwork-85"><span class="linenos"> 85</span></a><span class="sd">        controlled by the `learningRate`.</span>
</span><span id="NeuralNetwork-86"><a href="#NeuralNetwork-86"><span class="linenos"> 86</span></a>
</span><span id="NeuralNetwork-87"><a href="#NeuralNetwork-87"><span class="linenos"> 87</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork-88"><a href="#NeuralNetwork-88"><span class="linenos"> 88</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork-89"><a href="#NeuralNetwork-89"><span class="linenos"> 89</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="NeuralNetwork-90"><a href="#NeuralNetwork-90"><span class="linenos"> 90</span></a><span class="sd">            The inputs to the network, which should match the dimensions required for</span>
</span><span id="NeuralNetwork-91"><a href="#NeuralNetwork-91"><span class="linenos"> 91</span></a><span class="sd">            the input to the `forward` method of the first layer in the network.</span>
</span><span id="NeuralNetwork-92"><a href="#NeuralNetwork-92"><span class="linenos"> 92</span></a><span class="sd">        expectedOutputs : numpy.ndarray</span>
</span><span id="NeuralNetwork-93"><a href="#NeuralNetwork-93"><span class="linenos"> 93</span></a><span class="sd">            The desired values for the network&#39;s outputs. This parameter should have the same dimensions as that</span>
</span><span id="NeuralNetwork-94"><a href="#NeuralNetwork-94"><span class="linenos"> 94</span></a><span class="sd">            which would be produced by the network&#39;s `forward` method.</span>
</span><span id="NeuralNetwork-95"><a href="#NeuralNetwork-95"><span class="linenos"> 95</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="NeuralNetwork-96"><a href="#NeuralNetwork-96"><span class="linenos"> 96</span></a><span class="sd">            A cost function whose derivative is calculated with respect to the outputs of the network</span>
</span><span id="NeuralNetwork-97"><a href="#NeuralNetwork-97"><span class="linenos"> 97</span></a><span class="sd">            (i.e. the outputs of the last layer).</span>
</span><span id="NeuralNetwork-98"><a href="#NeuralNetwork-98"><span class="linenos"> 98</span></a><span class="sd">        learningRate : float</span>
</span><span id="NeuralNetwork-99"><a href="#NeuralNetwork-99"><span class="linenos"> 99</span></a><span class="sd">            Controls the amount by which the weights and biases of each layer are changed on each step. </span>
</span><span id="NeuralNetwork-100"><a href="#NeuralNetwork-100"><span class="linenos">100</span></a><span class="sd">            Both the weights and biases of each layer are updated at the same rate.</span>
</span><span id="NeuralNetwork-101"><a href="#NeuralNetwork-101"><span class="linenos">101</span></a>
</span><span id="NeuralNetwork-102"><a href="#NeuralNetwork-102"><span class="linenos">102</span></a><span class="sd">        Notes</span>
</span><span id="NeuralNetwork-103"><a href="#NeuralNetwork-103"><span class="linenos">103</span></a><span class="sd">        -----</span>
</span><span id="NeuralNetwork-104"><a href="#NeuralNetwork-104"><span class="linenos">104</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="NeuralNetwork-105"><a href="#NeuralNetwork-105"><span class="linenos">105</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-106"><a href="#NeuralNetwork-106"><span class="linenos">106</span></a>
</span><span id="NeuralNetwork-107"><a href="#NeuralNetwork-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-108"><a href="#NeuralNetwork-108"><span class="linenos">108</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</span><span id="NeuralNetwork-109"><a href="#NeuralNetwork-109"><span class="linenos">109</span></a>
</span><span id="NeuralNetwork-110"><a href="#NeuralNetwork-110"><span class="linenos">110</span></a>        <span class="c1">#Iterate over each layer, starting with the output layer</span>
</span><span id="NeuralNetwork-111"><a href="#NeuralNetwork-111"><span class="linenos">111</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="NeuralNetwork-112"><a href="#NeuralNetwork-112"><span class="linenos">112</span></a>            <span class="n">currentLayer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="NeuralNetwork-113"><a href="#NeuralNetwork-113"><span class="linenos">113</span></a>            <span class="c1">#Get the delta values for this layer</span>
</span><span id="NeuralNetwork-114"><a href="#NeuralNetwork-114"><span class="linenos">114</span></a>            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="NeuralNetwork-115"><a href="#NeuralNetwork-115"><span class="linenos">115</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getOutputDeltas</span><span class="p">(</span><span class="n">expectedOutputs</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">)</span>
</span><span id="NeuralNetwork-116"><a href="#NeuralNetwork-116"><span class="linenos">116</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getDeltas</span><span class="p">(</span><span class="n">currentLayer</span><span class="o">.</span><span class="n">outputDeltas</span><span class="p">)</span>
</span><span id="NeuralNetwork-117"><a href="#NeuralNetwork-117"><span class="linenos">117</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="NeuralNetwork-118"><a href="#NeuralNetwork-118"><span class="linenos">118</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getDeltas</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">inputDeltas</span><span class="p">)</span>
</span><span id="NeuralNetwork-119"><a href="#NeuralNetwork-119"><span class="linenos">119</span></a>
</span><span id="NeuralNetwork-120"><a href="#NeuralNetwork-120"><span class="linenos">120</span></a>            <span class="c1">#Get error gradients</span>
</span><span id="NeuralNetwork-121"><a href="#NeuralNetwork-121"><span class="linenos">121</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">currentLayer</span><span class="o">.</span><span class="n">getErrorGradients</span><span class="p">())</span>
</span><span id="NeuralNetwork-122"><a href="#NeuralNetwork-122"><span class="linenos">122</span></a>
</span><span id="NeuralNetwork-123"><a href="#NeuralNetwork-123"><span class="linenos">123</span></a>        <span class="c1">#Update all network weights and biases</span>
</span><span id="NeuralNetwork-124"><a href="#NeuralNetwork-124"><span class="linenos">124</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="NeuralNetwork-125"><a href="#NeuralNetwork-125"><span class="linenos">125</span></a>            <span class="n">currentLayer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="NeuralNetwork-126"><a href="#NeuralNetwork-126"><span class="linenos">126</span></a>            <span class="c1">#Reshape error gradients so that they can be added to the weights</span>
</span><span id="NeuralNetwork-127"><a href="#NeuralNetwork-127"><span class="linenos">127</span></a>            <span class="n">currentLayer</span><span class="o">.</span><span class="n">updateWeights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="NeuralNetwork-128"><a href="#NeuralNetwork-128"><span class="linenos">128</span></a>
</span><span id="NeuralNetwork-129"><a href="#NeuralNetwork-129"><span class="linenos">129</span></a>
</span><span id="NeuralNetwork-130"><a href="#NeuralNetwork-130"><span class="linenos">130</span></a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">batchSize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">checkGradients</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">testImages</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">testLabels</span> <span class="o">=</span> <span class="p">[]):</span>
</span><span id="NeuralNetwork-131"><a href="#NeuralNetwork-131"><span class="linenos">131</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Trains the network using stochastic gradient descent.</span>
</span><span id="NeuralNetwork-132"><a href="#NeuralNetwork-132"><span class="linenos">132</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-133"><a href="#NeuralNetwork-133"><span class="linenos">133</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork-134"><a href="#NeuralNetwork-134"><span class="linenos">134</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork-135"><a href="#NeuralNetwork-135"><span class="linenos">135</span></a><span class="sd">        X : numpy.ndarray</span>
</span><span id="NeuralNetwork-136"><a href="#NeuralNetwork-136"><span class="linenos">136</span></a><span class="sd">            The training inputs. The first dimension equals the total number of input samples (not batches).</span>
</span><span id="NeuralNetwork-137"><a href="#NeuralNetwork-137"><span class="linenos">137</span></a><span class="sd">        Y : numpy.ndarray</span>
</span><span id="NeuralNetwork-138"><a href="#NeuralNetwork-138"><span class="linenos">138</span></a><span class="sd">            The desired training outputs. The first dimension equals the total number of input samples (not batches).</span>
</span><span id="NeuralNetwork-139"><a href="#NeuralNetwork-139"><span class="linenos">139</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="NeuralNetwork-140"><a href="#NeuralNetwork-140"><span class="linenos">140</span></a><span class="sd">            A cost function to evaluate the performance of the neural network.</span>
</span><span id="NeuralNetwork-141"><a href="#NeuralNetwork-141"><span class="linenos">141</span></a><span class="sd">        batchSize : int</span>
</span><span id="NeuralNetwork-142"><a href="#NeuralNetwork-142"><span class="linenos">142</span></a><span class="sd">            The number of input samples to include in each batch.</span>
</span><span id="NeuralNetwork-143"><a href="#NeuralNetwork-143"><span class="linenos">143</span></a><span class="sd">        epochs : int, optional</span>
</span><span id="NeuralNetwork-144"><a href="#NeuralNetwork-144"><span class="linenos">144</span></a><span class="sd">            The number of times to iterate through the full training set. Defaults to 10.</span>
</span><span id="NeuralNetwork-145"><a href="#NeuralNetwork-145"><span class="linenos">145</span></a><span class="sd">        learningRate : float, optional</span>
</span><span id="NeuralNetwork-146"><a href="#NeuralNetwork-146"><span class="linenos">146</span></a><span class="sd">            Controls the rate of learning. A small value of `learningRate` will lead to slow training,</span>
</span><span id="NeuralNetwork-147"><a href="#NeuralNetwork-147"><span class="linenos">147</span></a><span class="sd">            but a large value will cause training instability. Defaults to 0.1.</span>
</span><span id="NeuralNetwork-148"><a href="#NeuralNetwork-148"><span class="linenos">148</span></a><span class="sd">        checkGradients : bool, optional</span>
</span><span id="NeuralNetwork-149"><a href="#NeuralNetwork-149"><span class="linenos">149</span></a><span class="sd">            Whether to perform gradient checking. This was used for verifying the gradients computed through</span>
</span><span id="NeuralNetwork-150"><a href="#NeuralNetwork-150"><span class="linenos">150</span></a><span class="sd">            backpropagation were correct, and is therefore unlikely to be useful to a user. Defaults to False.</span>
</span><span id="NeuralNetwork-151"><a href="#NeuralNetwork-151"><span class="linenos">151</span></a><span class="sd">        testImages : np.ndarray, optional</span>
</span><span id="NeuralNetwork-152"><a href="#NeuralNetwork-152"><span class="linenos">152</span></a><span class="sd">            Test data to evaluate the accuracy of the model on a test set during the training process.</span>
</span><span id="NeuralNetwork-153"><a href="#NeuralNetwork-153"><span class="linenos">153</span></a><span class="sd">            Defaults to [].</span>
</span><span id="NeuralNetwork-154"><a href="#NeuralNetwork-154"><span class="linenos">154</span></a><span class="sd">        testLabels : np.ndarray, optional</span>
</span><span id="NeuralNetwork-155"><a href="#NeuralNetwork-155"><span class="linenos">155</span></a><span class="sd">            Corresponding labels for `testImages` to evaluate the accuracy of the model on a test set during</span>
</span><span id="NeuralNetwork-156"><a href="#NeuralNetwork-156"><span class="linenos">156</span></a><span class="sd">            the training process. Defaults to [].</span>
</span><span id="NeuralNetwork-157"><a href="#NeuralNetwork-157"><span class="linenos">157</span></a>
</span><span id="NeuralNetwork-158"><a href="#NeuralNetwork-158"><span class="linenos">158</span></a><span class="sd">        Notes</span>
</span><span id="NeuralNetwork-159"><a href="#NeuralNetwork-159"><span class="linenos">159</span></a><span class="sd">        -----</span>
</span><span id="NeuralNetwork-160"><a href="#NeuralNetwork-160"><span class="linenos">160</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="NeuralNetwork-161"><a href="#NeuralNetwork-161"><span class="linenos">161</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-162"><a href="#NeuralNetwork-162"><span class="linenos">162</span></a>        <span class="c1">#If the cost function is CCE, the final activation must be softmax</span>
</span><span id="NeuralNetwork-163"><a href="#NeuralNetwork-163"><span class="linenos">163</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">costFunction</span><span class="p">,</span> <span class="n">CategoricalCrossEntropy</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activationFunction</span><span class="p">,</span> <span class="n">ActivationSoftmax</span><span class="p">):</span>
</span><span id="NeuralNetwork-164"><a href="#NeuralNetwork-164"><span class="linenos">164</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The Softmax function can only be used with CCE loss&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork-165"><a href="#NeuralNetwork-165"><span class="linenos">165</span></a>        
</span><span id="NeuralNetwork-166"><a href="#NeuralNetwork-166"><span class="linenos">166</span></a>        <span class="c1">#Get number of training examples</span>
</span><span id="NeuralNetwork-167"><a href="#NeuralNetwork-167"><span class="linenos">167</span></a>        <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NeuralNetwork-168"><a href="#NeuralNetwork-168"><span class="linenos">168</span></a>
</span><span id="NeuralNetwork-169"><a href="#NeuralNetwork-169"><span class="linenos">169</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">batchSize</span><span class="p">:</span>
</span><span id="NeuralNetwork-170"><a href="#NeuralNetwork-170"><span class="linenos">170</span></a>            <span class="n">batchSize</span> <span class="o">=</span> <span class="n">m</span>
</span><span id="NeuralNetwork-171"><a href="#NeuralNetwork-171"><span class="linenos">171</span></a>
</span><span id="NeuralNetwork-172"><a href="#NeuralNetwork-172"><span class="linenos">172</span></a>        <span class="n">startIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="NeuralNetwork-173"><a href="#NeuralNetwork-173"><span class="linenos">173</span></a>
</span><span id="NeuralNetwork-174"><a href="#NeuralNetwork-174"><span class="linenos">174</span></a>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
</span><span id="NeuralNetwork-175"><a href="#NeuralNetwork-175"><span class="linenos">175</span></a>            <span class="c1">#Iterate over all batches in the training set</span>
</span><span id="NeuralNetwork-176"><a href="#NeuralNetwork-176"><span class="linenos">176</span></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span> <span class="o">//</span> <span class="n">batchSize</span><span class="p">):</span>
</span><span id="NeuralNetwork-177"><a href="#NeuralNetwork-177"><span class="linenos">177</span></a>                <span class="k">try</span><span class="p">:</span>
</span><span id="NeuralNetwork-178"><a href="#NeuralNetwork-178"><span class="linenos">178</span></a>                    <span class="c1">#Get the current batch data</span>
</span><span id="NeuralNetwork-179"><a href="#NeuralNetwork-179"><span class="linenos">179</span></a>                    <span class="n">currentBatch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">startIndex</span><span class="p">:</span><span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">]</span>
</span><span id="NeuralNetwork-180"><a href="#NeuralNetwork-180"><span class="linenos">180</span></a>                    <span class="n">currentLabels</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">startIndex</span><span class="p">:</span><span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">]</span>
</span><span id="NeuralNetwork-181"><a href="#NeuralNetwork-181"><span class="linenos">181</span></a>
</span><span id="NeuralNetwork-182"><a href="#NeuralNetwork-182"><span class="linenos">182</span></a>                    <span class="c1">#Perform a forward pass using the input data</span>
</span><span id="NeuralNetwork-183"><a href="#NeuralNetwork-183"><span class="linenos">183</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">)</span>
</span><span id="NeuralNetwork-184"><a href="#NeuralNetwork-184"><span class="linenos">184</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">updateWeights</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">,</span> <span class="n">currentLabels</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="NeuralNetwork-185"><a href="#NeuralNetwork-185"><span class="linenos">185</span></a>                    <span class="c1">#Print cost for mini batch</span>
</span><span id="NeuralNetwork-186"><a href="#NeuralNetwork-186"><span class="linenos">186</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cost: </span><span class="si">{</span><span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">currentLabels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork-187"><a href="#NeuralNetwork-187"><span class="linenos">187</span></a>
</span><span id="NeuralNetwork-188"><a href="#NeuralNetwork-188"><span class="linenos">188</span></a>                    <span class="c1">#Numerically estimate the gradients to verify backprop implementation</span>
</span><span id="NeuralNetwork-189"><a href="#NeuralNetwork-189"><span class="linenos">189</span></a>                    <span class="k">if</span> <span class="n">checkGradients</span><span class="p">:</span>
</span><span id="NeuralNetwork-190"><a href="#NeuralNetwork-190"><span class="linenos">190</span></a>                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error gradients (by backprop): </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork-191"><a href="#NeuralNetwork-191"><span class="linenos">191</span></a>                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Numerically estimated gradients: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">getEstimatedGradients</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">,</span><span class="w"> </span><span class="n">currentLabels</span><span class="p">,</span><span class="w"> </span><span class="n">costFunction</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork-192"><a href="#NeuralNetwork-192"><span class="linenos">192</span></a>
</span><span id="NeuralNetwork-193"><a href="#NeuralNetwork-193"><span class="linenos">193</span></a>                    <span class="c1"># Evaluate accuracy on test set</span>
</span><span id="NeuralNetwork-194"><a href="#NeuralNetwork-194"><span class="linenos">194</span></a><span class="w">                    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="NeuralNetwork-195"><a href="#NeuralNetwork-195"><span class="linenos">195</span></a><span class="sd">                    if testImages != [] and testLabels != []:</span>
</span><span id="NeuralNetwork-196"><a href="#NeuralNetwork-196"><span class="linenos">196</span></a><span class="sd">                        self.forward(testImages)</span>
</span><span id="NeuralNetwork-197"><a href="#NeuralNetwork-197"><span class="linenos">197</span></a><span class="sd">                        testOutputs = np.argmax(self.getOutputs(), -1)</span>
</span><span id="NeuralNetwork-198"><a href="#NeuralNetwork-198"><span class="linenos">198</span></a><span class="sd">                        matchingCount = 0</span>
</span><span id="NeuralNetwork-199"><a href="#NeuralNetwork-199"><span class="linenos">199</span></a><span class="sd">                        for i in range(testOutputs.size):</span>
</span><span id="NeuralNetwork-200"><a href="#NeuralNetwork-200"><span class="linenos">200</span></a><span class="sd">                            if testOutputs[i] == np.argmax(testLabels[i], -1):</span>
</span><span id="NeuralNetwork-201"><a href="#NeuralNetwork-201"><span class="linenos">201</span></a><span class="sd">                                matchingCount += 1</span>
</span><span id="NeuralNetwork-202"><a href="#NeuralNetwork-202"><span class="linenos">202</span></a>
</span><span id="NeuralNetwork-203"><a href="#NeuralNetwork-203"><span class="linenos">203</span></a><span class="sd">                        print(&quot;Accuracy on test set: {}%&quot;.format((matchingCount / testOutputs.size) * 100))</span>
</span><span id="NeuralNetwork-204"><a href="#NeuralNetwork-204"><span class="linenos">204</span></a><span class="sd">                    &#39;&#39;&#39;</span>
</span><span id="NeuralNetwork-205"><a href="#NeuralNetwork-205"><span class="linenos">205</span></a>                    <span class="n">startIndex</span> <span class="o">+=</span> <span class="n">batchSize</span>
</span><span id="NeuralNetwork-206"><a href="#NeuralNetwork-206"><span class="linenos">206</span></a>
</span><span id="NeuralNetwork-207"><a href="#NeuralNetwork-207"><span class="linenos">207</span></a>                    <span class="k">if</span> <span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span> <span class="o">&gt;</span> <span class="n">m</span><span class="p">:</span>
</span><span id="NeuralNetwork-208"><a href="#NeuralNetwork-208"><span class="linenos">208</span></a>                        <span class="n">startIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="NeuralNetwork-209"><a href="#NeuralNetwork-209"><span class="linenos">209</span></a>                    
</span><span id="NeuralNetwork-210"><a href="#NeuralNetwork-210"><span class="linenos">210</span></a>                <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span><span id="NeuralNetwork-211"><a href="#NeuralNetwork-211"><span class="linenos">211</span></a>                    <span class="k">break</span>
</span><span id="NeuralNetwork-212"><a href="#NeuralNetwork-212"><span class="linenos">212</span></a>
</span><span id="NeuralNetwork-213"><a href="#NeuralNetwork-213"><span class="linenos">213</span></a>    <span class="k">def</span> <span class="nf">getEstimatedGradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">):</span>
</span><span id="NeuralNetwork-214"><a href="#NeuralNetwork-214"><span class="linenos">214</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Numerically estimates the partial derivatives of the cost function w.r.t each weight.</span>
</span><span id="NeuralNetwork-215"><a href="#NeuralNetwork-215"><span class="linenos">215</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-216"><a href="#NeuralNetwork-216"><span class="linenos">216</span></a><span class="sd">        This is used in gradient checking as an alternative way of computing gradients other</span>
</span><span id="NeuralNetwork-217"><a href="#NeuralNetwork-217"><span class="linenos">217</span></a><span class="sd">        than backpropagation, but is unlikely to be useful to a user.</span>
</span><span id="NeuralNetwork-218"><a href="#NeuralNetwork-218"><span class="linenos">218</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-219"><a href="#NeuralNetwork-219"><span class="linenos">219</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork-220"><a href="#NeuralNetwork-220"><span class="linenos">220</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork-221"><a href="#NeuralNetwork-221"><span class="linenos">221</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="NeuralNetwork-222"><a href="#NeuralNetwork-222"><span class="linenos">222</span></a><span class="sd">            The inputs used for evaluating cost, and hence estimating derivatives.</span>
</span><span id="NeuralNetwork-223"><a href="#NeuralNetwork-223"><span class="linenos">223</span></a><span class="sd">        yReal : numpy.ndarray</span>
</span><span id="NeuralNetwork-224"><a href="#NeuralNetwork-224"><span class="linenos">224</span></a><span class="sd">            The correct outputs for the given inputs, used for evaluating cost.</span>
</span><span id="NeuralNetwork-225"><a href="#NeuralNetwork-225"><span class="linenos">225</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="NeuralNetwork-226"><a href="#NeuralNetwork-226"><span class="linenos">226</span></a><span class="sd">            The cost function used for taking derivatives.</span>
</span><span id="NeuralNetwork-227"><a href="#NeuralNetwork-227"><span class="linenos">227</span></a>
</span><span id="NeuralNetwork-228"><a href="#NeuralNetwork-228"><span class="linenos">228</span></a><span class="sd">        Returns</span>
</span><span id="NeuralNetwork-229"><a href="#NeuralNetwork-229"><span class="linenos">229</span></a><span class="sd">        -------</span>
</span><span id="NeuralNetwork-230"><a href="#NeuralNetwork-230"><span class="linenos">230</span></a><span class="sd">        array-like</span>
</span><span id="NeuralNetwork-231"><a href="#NeuralNetwork-231"><span class="linenos">231</span></a><span class="sd">            The estimated gradients with respect to the weights and biases of each layer.</span>
</span><span id="NeuralNetwork-232"><a href="#NeuralNetwork-232"><span class="linenos">232</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork-233"><a href="#NeuralNetwork-233"><span class="linenos">233</span></a><span class="sd">        Notes</span>
</span><span id="NeuralNetwork-234"><a href="#NeuralNetwork-234"><span class="linenos">234</span></a><span class="sd">        -----</span>
</span><span id="NeuralNetwork-235"><a href="#NeuralNetwork-235"><span class="linenos">235</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="NeuralNetwork-236"><a href="#NeuralNetwork-236"><span class="linenos">236</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork-237"><a href="#NeuralNetwork-237"><span class="linenos">237</span></a>
</span><span id="NeuralNetwork-238"><a href="#NeuralNetwork-238"><span class="linenos">238</span></a>        <span class="c1">#Assemble a list of computed gradient matrices</span>
</span><span id="NeuralNetwork-239"><a href="#NeuralNetwork-239"><span class="linenos">239</span></a>        <span class="n">estimatedGradients</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</span><span id="NeuralNetwork-240"><a href="#NeuralNetwork-240"><span class="linenos">240</span></a>
</span><span id="NeuralNetwork-241"><a href="#NeuralNetwork-241"><span class="linenos">241</span></a>        <span class="c1"># Iterate over each bias parameter in each layer</span>
</span><span id="NeuralNetwork-242"><a href="#NeuralNetwork-242"><span class="linenos">242</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="NeuralNetwork-243"><a href="#NeuralNetwork-243"><span class="linenos">243</span></a>            <span class="n">flattenedBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork-244"><a href="#NeuralNetwork-244"><span class="linenos">244</span></a>            <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">flattenedBiases</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span id="NeuralNetwork-245"><a href="#NeuralNetwork-245"><span class="linenos">245</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flattenedBiases</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span id="NeuralNetwork-246"><a href="#NeuralNetwork-246"><span class="linenos">246</span></a>                <span class="c1">#A small value so that the gradient can be calculated between 2 points</span>
</span><span id="NeuralNetwork-247"><a href="#NeuralNetwork-247"><span class="linenos">247</span></a>                <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span><span id="NeuralNetwork-248"><a href="#NeuralNetwork-248"><span class="linenos">248</span></a>                <span class="c1">#Copy the biases matrix so that it can be replaced once gradient checking completes</span>
</span><span id="NeuralNetwork-249"><a href="#NeuralNetwork-249"><span class="linenos">249</span></a>                <span class="n">biasesCopy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="NeuralNetwork-250"><a href="#NeuralNetwork-250"><span class="linenos">250</span></a>                <span class="n">flattenedBiases</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork-251"><a href="#NeuralNetwork-251"><span class="linenos">251</span></a>
</span><span id="NeuralNetwork-252"><a href="#NeuralNetwork-252"><span class="linenos">252</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-253"><a href="#NeuralNetwork-253"><span class="linenos">253</span></a>                <span class="n">gradPlus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork-254"><a href="#NeuralNetwork-254"><span class="linenos">254</span></a>                <span class="n">flattenedBiases</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork-255"><a href="#NeuralNetwork-255"><span class="linenos">255</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-256"><a href="#NeuralNetwork-256"><span class="linenos">256</span></a>                <span class="n">gradMinus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork-257"><a href="#NeuralNetwork-257"><span class="linenos">257</span></a>
</span><span id="NeuralNetwork-258"><a href="#NeuralNetwork-258"><span class="linenos">258</span></a>                <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradPlus</span> <span class="o">-</span> <span class="n">gradMinus</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span><span class="p">)</span>
</span><span id="NeuralNetwork-259"><a href="#NeuralNetwork-259"><span class="linenos">259</span></a>
</span><span id="NeuralNetwork-260"><a href="#NeuralNetwork-260"><span class="linenos">260</span></a>                <span class="c1">#Re-instate original layer biases</span>
</span><span id="NeuralNetwork-261"><a href="#NeuralNetwork-261"><span class="linenos">261</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">biasesCopy</span>
</span><span id="NeuralNetwork-262"><a href="#NeuralNetwork-262"><span class="linenos">262</span></a>                <span class="n">flattenedBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork-263"><a href="#NeuralNetwork-263"><span class="linenos">263</span></a>
</span><span id="NeuralNetwork-264"><a href="#NeuralNetwork-264"><span class="linenos">264</span></a>        <span class="c1">#Iterate over each weight parameter in each layer</span>
</span><span id="NeuralNetwork-265"><a href="#NeuralNetwork-265"><span class="linenos">265</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="NeuralNetwork-266"><a href="#NeuralNetwork-266"><span class="linenos">266</span></a>            <span class="n">flattenedWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork-267"><a href="#NeuralNetwork-267"><span class="linenos">267</span></a>            <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">flattenedWeights</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span id="NeuralNetwork-268"><a href="#NeuralNetwork-268"><span class="linenos">268</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flattenedWeights</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span id="NeuralNetwork-269"><a href="#NeuralNetwork-269"><span class="linenos">269</span></a>                <span class="c1">#A small value so that the gradient can be calculated between 2 points</span>
</span><span id="NeuralNetwork-270"><a href="#NeuralNetwork-270"><span class="linenos">270</span></a>                <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span><span id="NeuralNetwork-271"><a href="#NeuralNetwork-271"><span class="linenos">271</span></a>                <span class="c1">#Copy the weights matrix so that it can be replaced once gradient checking completes</span>
</span><span id="NeuralNetwork-272"><a href="#NeuralNetwork-272"><span class="linenos">272</span></a>                <span class="n">weightsCopy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="NeuralNetwork-273"><a href="#NeuralNetwork-273"><span class="linenos">273</span></a>                <span class="n">flattenedWeights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork-274"><a href="#NeuralNetwork-274"><span class="linenos">274</span></a>
</span><span id="NeuralNetwork-275"><a href="#NeuralNetwork-275"><span class="linenos">275</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-276"><a href="#NeuralNetwork-276"><span class="linenos">276</span></a>                <span class="n">gradPlus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork-277"><a href="#NeuralNetwork-277"><span class="linenos">277</span></a>                <span class="n">flattenedWeights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork-278"><a href="#NeuralNetwork-278"><span class="linenos">278</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork-279"><a href="#NeuralNetwork-279"><span class="linenos">279</span></a>                <span class="n">gradMinus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork-280"><a href="#NeuralNetwork-280"><span class="linenos">280</span></a>
</span><span id="NeuralNetwork-281"><a href="#NeuralNetwork-281"><span class="linenos">281</span></a>                <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradPlus</span> <span class="o">-</span> <span class="n">gradMinus</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span><span class="p">)</span>
</span><span id="NeuralNetwork-282"><a href="#NeuralNetwork-282"><span class="linenos">282</span></a>
</span><span id="NeuralNetwork-283"><a href="#NeuralNetwork-283"><span class="linenos">283</span></a>                <span class="c1">#Re-instate original layer weights</span>
</span><span id="NeuralNetwork-284"><a href="#NeuralNetwork-284"><span class="linenos">284</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weightsCopy</span>
</span><span id="NeuralNetwork-285"><a href="#NeuralNetwork-285"><span class="linenos">285</span></a>                <span class="n">flattenedWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork-286"><a href="#NeuralNetwork-286"><span class="linenos">286</span></a>
</span><span id="NeuralNetwork-287"><a href="#NeuralNetwork-287"><span class="linenos">287</span></a>        <span class="k">return</span> <span class="n">estimatedGradients</span>
</span></pre></div>


            <div class="docstring"><p>Represents a neural network, which may be fully connected or convolutional.</p>

<h6 id="attributes">Attributes</h6>

<ul>
<li><strong>name</strong> (str):
A string identifier for the neural network e.g. for naming weights files.</li>
<li><strong>layers</strong> (array_like):
A list of layers that comprise the neural network. Inputs are propagated through this set of layers
sequentially from start to finish when the <code><a href="#NeuralNetwork.forward">forward</a></code> method is called.</li>
</ul>
</div>


                            <div id="NeuralNetwork.__init__" class="classattr">
                                        <input id="NeuralNetwork.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">NeuralNetwork</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">name</span>, </span><span class="param"><span class="n">layers</span></span>)</span>

                <label class="view-source-button" for="NeuralNetwork.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.__init__-19"><a href="#NeuralNetwork.__init__-19"><span class="linenos">19</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
</span><span id="NeuralNetwork.__init__-20"><a href="#NeuralNetwork.__init__-20"><span class="linenos">20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
</span><span id="NeuralNetwork.__init__-21"><a href="#NeuralNetwork.__init__-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">layers</span>
</span></pre></div>


    

                            </div>
                            <div id="NeuralNetwork.forward" class="classattr">
                                        <input id="NeuralNetwork.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputs</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.forward-23"><a href="#NeuralNetwork.forward-23"><span class="linenos">23</span></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span><span id="NeuralNetwork.forward-24"><a href="#NeuralNetwork.forward-24"><span class="linenos">24</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Performs a forward pass of the network.</span>
</span><span id="NeuralNetwork.forward-25"><a href="#NeuralNetwork.forward-25"><span class="linenos">25</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.forward-26"><a href="#NeuralNetwork.forward-26"><span class="linenos">26</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork.forward-27"><a href="#NeuralNetwork.forward-27"><span class="linenos">27</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork.forward-28"><a href="#NeuralNetwork.forward-28"><span class="linenos">28</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="NeuralNetwork.forward-29"><a href="#NeuralNetwork.forward-29"><span class="linenos">29</span></a><span class="sd">            The inputs to the network, which should match the dimensions required for</span>
</span><span id="NeuralNetwork.forward-30"><a href="#NeuralNetwork.forward-30"><span class="linenos">30</span></a><span class="sd">            the input to the `forward` method of the first layer in the network.</span>
</span><span id="NeuralNetwork.forward-31"><a href="#NeuralNetwork.forward-31"><span class="linenos">31</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.forward-32"><a href="#NeuralNetwork.forward-32"><span class="linenos">32</span></a>        <span class="c1"># Initialise inputs to starting values</span>
</span><span id="NeuralNetwork.forward-33"><a href="#NeuralNetwork.forward-33"><span class="linenos">33</span></a>        <span class="n">nextInputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.forward-34"><a href="#NeuralNetwork.forward-34"><span class="linenos">34</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="NeuralNetwork.forward-35"><a href="#NeuralNetwork.forward-35"><span class="linenos">35</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">nextInputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.forward-36"><a href="#NeuralNetwork.forward-36"><span class="linenos">36</span></a>            <span class="c1"># Initialise inputs of next layer as outputs from current layer</span>
</span><span id="NeuralNetwork.forward-37"><a href="#NeuralNetwork.forward-37"><span class="linenos">37</span></a>            <span class="n">nextInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span></pre></div>


            <div class="docstring"><p>Performs a forward pass of the network.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>inputs</strong> (numpy.ndarray):
The inputs to the network, which should match the dimensions required for
the input to the <code><a href="#NeuralNetwork.forward">forward</a></code> method of the first layer in the network.</li>
</ul>
</div>


                            </div>
                            <div id="NeuralNetwork.getOutputs" class="classattr">
                                        <input id="NeuralNetwork.getOutputs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">getOutputs</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.getOutputs-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.getOutputs"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.getOutputs-39"><a href="#NeuralNetwork.getOutputs-39"><span class="linenos">39</span></a>    <span class="k">def</span> <span class="nf">getOutputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="NeuralNetwork.getOutputs-40"><a href="#NeuralNetwork.getOutputs-40"><span class="linenos">40</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the outputs from the last layer of the network produced by the</span>
</span><span id="NeuralNetwork.getOutputs-41"><a href="#NeuralNetwork.getOutputs-41"><span class="linenos">41</span></a><span class="sd">        most recent call to `forward`.</span>
</span><span id="NeuralNetwork.getOutputs-42"><a href="#NeuralNetwork.getOutputs-42"><span class="linenos">42</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.getOutputs-43"><a href="#NeuralNetwork.getOutputs-43"><span class="linenos">43</span></a><span class="sd">        Returns</span>
</span><span id="NeuralNetwork.getOutputs-44"><a href="#NeuralNetwork.getOutputs-44"><span class="linenos">44</span></a><span class="sd">        -------</span>
</span><span id="NeuralNetwork.getOutputs-45"><a href="#NeuralNetwork.getOutputs-45"><span class="linenos">45</span></a><span class="sd">        numpy.ndarray</span>
</span><span id="NeuralNetwork.getOutputs-46"><a href="#NeuralNetwork.getOutputs-46"><span class="linenos">46</span></a><span class="sd">            The outputs of the last layer, whose dimensions match those of the</span>
</span><span id="NeuralNetwork.getOutputs-47"><a href="#NeuralNetwork.getOutputs-47"><span class="linenos">47</span></a><span class="sd">            output of the `forward` method in the last layer of the network.</span>
</span><span id="NeuralNetwork.getOutputs-48"><a href="#NeuralNetwork.getOutputs-48"><span class="linenos">48</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.getOutputs-49"><a href="#NeuralNetwork.getOutputs-49"><span class="linenos">49</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span></pre></div>


            <div class="docstring"><p>Returns the outputs from the last layer of the network produced by the
most recent call to <code><a href="#NeuralNetwork.forward">forward</a></code>.</p>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>numpy.ndarray</strong>: The outputs of the last layer, whose dimensions match those of the
output of the <code><a href="#NeuralNetwork.forward">forward</a></code> method in the last layer of the network.</li>
</ul>
</div>


                            </div>
                            <div id="NeuralNetwork.saveWeights" class="classattr">
                                        <input id="NeuralNetwork.saveWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">saveWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.saveWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.saveWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.saveWeights-51"><a href="#NeuralNetwork.saveWeights-51"><span class="linenos">51</span></a>    <span class="k">def</span> <span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="NeuralNetwork.saveWeights-52"><a href="#NeuralNetwork.saveWeights-52"><span class="linenos">52</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves the current weights for the network in a pickle file.</span>
</span><span id="NeuralNetwork.saveWeights-53"><a href="#NeuralNetwork.saveWeights-53"><span class="linenos">53</span></a>
</span><span id="NeuralNetwork.saveWeights-54"><a href="#NeuralNetwork.saveWeights-54"><span class="linenos">54</span></a><span class="sd">        The file is named as ``{network_name}.pkl``, and saved in the</span>
</span><span id="NeuralNetwork.saveWeights-55"><a href="#NeuralNetwork.saveWeights-55"><span class="linenos">55</span></a><span class="sd">        ``deeplearningnumpy/data/`` directory. </span>
</span><span id="NeuralNetwork.saveWeights-56"><a href="#NeuralNetwork.saveWeights-56"><span class="linenos">56</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.saveWeights-57"><a href="#NeuralNetwork.saveWeights-57"><span class="linenos">57</span></a>        <span class="n">fileName</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;deeplearningnumpy/data/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
</span><span id="NeuralNetwork.saveWeights-58"><a href="#NeuralNetwork.saveWeights-58"><span class="linenos">58</span></a>        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fileHandle</span><span class="p">:</span>
</span><span id="NeuralNetwork.saveWeights-59"><a href="#NeuralNetwork.saveWeights-59"><span class="linenos">59</span></a>            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">,</span> <span class="n">fileHandle</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Saves the current weights for the network in a pickle file.</p>

<p>The file is named as <code>{network_name}.pkl</code>, and saved in the
<code>deeplearningnumpy/data/</code> directory.</p>
</div>


                            </div>
                            <div id="NeuralNetwork.loadWeights" class="classattr">
                                        <input id="NeuralNetwork.loadWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">loadWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.loadWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.loadWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.loadWeights-61"><a href="#NeuralNetwork.loadWeights-61"><span class="linenos">61</span></a>    <span class="k">def</span> <span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="NeuralNetwork.loadWeights-62"><a href="#NeuralNetwork.loadWeights-62"><span class="linenos">62</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the weights for this network from the appropriate pickle file (if it exists).</span>
</span><span id="NeuralNetwork.loadWeights-63"><a href="#NeuralNetwork.loadWeights-63"><span class="linenos">63</span></a>
</span><span id="NeuralNetwork.loadWeights-64"><a href="#NeuralNetwork.loadWeights-64"><span class="linenos">64</span></a><span class="sd">        The file that is read is ``deeplearningnumpy/data/{network_name}.pkl``.</span>
</span><span id="NeuralNetwork.loadWeights-65"><a href="#NeuralNetwork.loadWeights-65"><span class="linenos">65</span></a>
</span><span id="NeuralNetwork.loadWeights-66"><a href="#NeuralNetwork.loadWeights-66"><span class="linenos">66</span></a><span class="sd">        Raises</span>
</span><span id="NeuralNetwork.loadWeights-67"><a href="#NeuralNetwork.loadWeights-67"><span class="linenos">67</span></a><span class="sd">        ------</span>
</span><span id="NeuralNetwork.loadWeights-68"><a href="#NeuralNetwork.loadWeights-68"><span class="linenos">68</span></a><span class="sd">        FileNotFoundError</span>
</span><span id="NeuralNetwork.loadWeights-69"><a href="#NeuralNetwork.loadWeights-69"><span class="linenos">69</span></a><span class="sd">            If the file for the network does not exist.</span>
</span><span id="NeuralNetwork.loadWeights-70"><a href="#NeuralNetwork.loadWeights-70"><span class="linenos">70</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.loadWeights-71"><a href="#NeuralNetwork.loadWeights-71"><span class="linenos">71</span></a>        <span class="n">fileName</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;deeplearningnumpy/data/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
</span><span id="NeuralNetwork.loadWeights-72"><a href="#NeuralNetwork.loadWeights-72"><span class="linenos">72</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">fileName</span><span class="p">):</span>
</span><span id="NeuralNetwork.loadWeights-73"><a href="#NeuralNetwork.loadWeights-73"><span class="linenos">73</span></a>            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights for network &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&#39; could not be loaded - have you trained your model?&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork.loadWeights-74"><a href="#NeuralNetwork.loadWeights-74"><span class="linenos">74</span></a>
</span><span id="NeuralNetwork.loadWeights-75"><a href="#NeuralNetwork.loadWeights-75"><span class="linenos">75</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="NeuralNetwork.loadWeights-76"><a href="#NeuralNetwork.loadWeights-76"><span class="linenos">76</span></a>            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fileHandle</span><span class="p">:</span>
</span><span id="NeuralNetwork.loadWeights-77"><a href="#NeuralNetwork.loadWeights-77"><span class="linenos">77</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileHandle</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Loads the weights for this network from the appropriate pickle file (if it exists).</p>

<p>The file that is read is <code>deeplearningnumpy/data/{network_name}.pkl</code>.</p>

<h6 id="raises">Raises</h6>

<ul>
<li><strong>FileNotFoundError</strong>: If the file for the network does not exist.</li>
</ul>
</div>


                            </div>
                            <div id="NeuralNetwork.updateWeights" class="classattr">
                                        <input id="NeuralNetwork.updateWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">updateWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputs</span>, </span><span class="param"><span class="n">expectedOutputs</span>, </span><span class="param"><span class="n">costFunction</span>, </span><span class="param"><span class="n">learningRate</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.updateWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.updateWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.updateWeights-79"><a href="#NeuralNetwork.updateWeights-79"><span class="linenos"> 79</span></a>    <span class="k">def</span> <span class="nf">updateWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">expectedOutputs</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="NeuralNetwork.updateWeights-80"><a href="#NeuralNetwork.updateWeights-80"><span class="linenos"> 80</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform weight updates for all layers in the network.</span>
</span><span id="NeuralNetwork.updateWeights-81"><a href="#NeuralNetwork.updateWeights-81"><span class="linenos"> 81</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.updateWeights-82"><a href="#NeuralNetwork.updateWeights-82"><span class="linenos"> 82</span></a><span class="sd">        A forward pass is made prior to updating weights to ensure that the layer outputs</span>
</span><span id="NeuralNetwork.updateWeights-83"><a href="#NeuralNetwork.updateWeights-83"><span class="linenos"> 83</span></a><span class="sd">        are those produced by inputting `inputs`. The weights are then updated to</span>
</span><span id="NeuralNetwork.updateWeights-84"><a href="#NeuralNetwork.updateWeights-84"><span class="linenos"> 84</span></a><span class="sd">        bring the outputs closer to `expectedOutputs`, according to `costFunction` and </span>
</span><span id="NeuralNetwork.updateWeights-85"><a href="#NeuralNetwork.updateWeights-85"><span class="linenos"> 85</span></a><span class="sd">        controlled by the `learningRate`.</span>
</span><span id="NeuralNetwork.updateWeights-86"><a href="#NeuralNetwork.updateWeights-86"><span class="linenos"> 86</span></a>
</span><span id="NeuralNetwork.updateWeights-87"><a href="#NeuralNetwork.updateWeights-87"><span class="linenos"> 87</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork.updateWeights-88"><a href="#NeuralNetwork.updateWeights-88"><span class="linenos"> 88</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork.updateWeights-89"><a href="#NeuralNetwork.updateWeights-89"><span class="linenos"> 89</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="NeuralNetwork.updateWeights-90"><a href="#NeuralNetwork.updateWeights-90"><span class="linenos"> 90</span></a><span class="sd">            The inputs to the network, which should match the dimensions required for</span>
</span><span id="NeuralNetwork.updateWeights-91"><a href="#NeuralNetwork.updateWeights-91"><span class="linenos"> 91</span></a><span class="sd">            the input to the `forward` method of the first layer in the network.</span>
</span><span id="NeuralNetwork.updateWeights-92"><a href="#NeuralNetwork.updateWeights-92"><span class="linenos"> 92</span></a><span class="sd">        expectedOutputs : numpy.ndarray</span>
</span><span id="NeuralNetwork.updateWeights-93"><a href="#NeuralNetwork.updateWeights-93"><span class="linenos"> 93</span></a><span class="sd">            The desired values for the network&#39;s outputs. This parameter should have the same dimensions as that</span>
</span><span id="NeuralNetwork.updateWeights-94"><a href="#NeuralNetwork.updateWeights-94"><span class="linenos"> 94</span></a><span class="sd">            which would be produced by the network&#39;s `forward` method.</span>
</span><span id="NeuralNetwork.updateWeights-95"><a href="#NeuralNetwork.updateWeights-95"><span class="linenos"> 95</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="NeuralNetwork.updateWeights-96"><a href="#NeuralNetwork.updateWeights-96"><span class="linenos"> 96</span></a><span class="sd">            A cost function whose derivative is calculated with respect to the outputs of the network</span>
</span><span id="NeuralNetwork.updateWeights-97"><a href="#NeuralNetwork.updateWeights-97"><span class="linenos"> 97</span></a><span class="sd">            (i.e. the outputs of the last layer).</span>
</span><span id="NeuralNetwork.updateWeights-98"><a href="#NeuralNetwork.updateWeights-98"><span class="linenos"> 98</span></a><span class="sd">        learningRate : float</span>
</span><span id="NeuralNetwork.updateWeights-99"><a href="#NeuralNetwork.updateWeights-99"><span class="linenos"> 99</span></a><span class="sd">            Controls the amount by which the weights and biases of each layer are changed on each step. </span>
</span><span id="NeuralNetwork.updateWeights-100"><a href="#NeuralNetwork.updateWeights-100"><span class="linenos">100</span></a><span class="sd">            Both the weights and biases of each layer are updated at the same rate.</span>
</span><span id="NeuralNetwork.updateWeights-101"><a href="#NeuralNetwork.updateWeights-101"><span class="linenos">101</span></a>
</span><span id="NeuralNetwork.updateWeights-102"><a href="#NeuralNetwork.updateWeights-102"><span class="linenos">102</span></a><span class="sd">        Notes</span>
</span><span id="NeuralNetwork.updateWeights-103"><a href="#NeuralNetwork.updateWeights-103"><span class="linenos">103</span></a><span class="sd">        -----</span>
</span><span id="NeuralNetwork.updateWeights-104"><a href="#NeuralNetwork.updateWeights-104"><span class="linenos">104</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="NeuralNetwork.updateWeights-105"><a href="#NeuralNetwork.updateWeights-105"><span class="linenos">105</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.updateWeights-106"><a href="#NeuralNetwork.updateWeights-106"><span class="linenos">106</span></a>
</span><span id="NeuralNetwork.updateWeights-107"><a href="#NeuralNetwork.updateWeights-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.updateWeights-108"><a href="#NeuralNetwork.updateWeights-108"><span class="linenos">108</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</span><span id="NeuralNetwork.updateWeights-109"><a href="#NeuralNetwork.updateWeights-109"><span class="linenos">109</span></a>
</span><span id="NeuralNetwork.updateWeights-110"><a href="#NeuralNetwork.updateWeights-110"><span class="linenos">110</span></a>        <span class="c1">#Iterate over each layer, starting with the output layer</span>
</span><span id="NeuralNetwork.updateWeights-111"><a href="#NeuralNetwork.updateWeights-111"><span class="linenos">111</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="NeuralNetwork.updateWeights-112"><a href="#NeuralNetwork.updateWeights-112"><span class="linenos">112</span></a>            <span class="n">currentLayer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="NeuralNetwork.updateWeights-113"><a href="#NeuralNetwork.updateWeights-113"><span class="linenos">113</span></a>            <span class="c1">#Get the delta values for this layer</span>
</span><span id="NeuralNetwork.updateWeights-114"><a href="#NeuralNetwork.updateWeights-114"><span class="linenos">114</span></a>            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="NeuralNetwork.updateWeights-115"><a href="#NeuralNetwork.updateWeights-115"><span class="linenos">115</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getOutputDeltas</span><span class="p">(</span><span class="n">expectedOutputs</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">)</span>
</span><span id="NeuralNetwork.updateWeights-116"><a href="#NeuralNetwork.updateWeights-116"><span class="linenos">116</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getDeltas</span><span class="p">(</span><span class="n">currentLayer</span><span class="o">.</span><span class="n">outputDeltas</span><span class="p">)</span>
</span><span id="NeuralNetwork.updateWeights-117"><a href="#NeuralNetwork.updateWeights-117"><span class="linenos">117</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="NeuralNetwork.updateWeights-118"><a href="#NeuralNetwork.updateWeights-118"><span class="linenos">118</span></a>                <span class="n">currentLayer</span><span class="o">.</span><span class="n">getDeltas</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">inputDeltas</span><span class="p">)</span>
</span><span id="NeuralNetwork.updateWeights-119"><a href="#NeuralNetwork.updateWeights-119"><span class="linenos">119</span></a>
</span><span id="NeuralNetwork.updateWeights-120"><a href="#NeuralNetwork.updateWeights-120"><span class="linenos">120</span></a>            <span class="c1">#Get error gradients</span>
</span><span id="NeuralNetwork.updateWeights-121"><a href="#NeuralNetwork.updateWeights-121"><span class="linenos">121</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">currentLayer</span><span class="o">.</span><span class="n">getErrorGradients</span><span class="p">())</span>
</span><span id="NeuralNetwork.updateWeights-122"><a href="#NeuralNetwork.updateWeights-122"><span class="linenos">122</span></a>
</span><span id="NeuralNetwork.updateWeights-123"><a href="#NeuralNetwork.updateWeights-123"><span class="linenos">123</span></a>        <span class="c1">#Update all network weights and biases</span>
</span><span id="NeuralNetwork.updateWeights-124"><a href="#NeuralNetwork.updateWeights-124"><span class="linenos">124</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="NeuralNetwork.updateWeights-125"><a href="#NeuralNetwork.updateWeights-125"><span class="linenos">125</span></a>            <span class="n">currentLayer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="NeuralNetwork.updateWeights-126"><a href="#NeuralNetwork.updateWeights-126"><span class="linenos">126</span></a>            <span class="c1">#Reshape error gradients so that they can be added to the weights</span>
</span><span id="NeuralNetwork.updateWeights-127"><a href="#NeuralNetwork.updateWeights-127"><span class="linenos">127</span></a>            <span class="n">currentLayer</span><span class="o">.</span><span class="n">updateWeights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">learningRate</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Perform weight updates for all layers in the network.</p>

<p>A forward pass is made prior to updating weights to ensure that the layer outputs
are those produced by inputting <code>inputs</code>. The weights are then updated to
bring the outputs closer to <code>expectedOutputs</code>, according to <code>costFunction</code> and 
controlled by the <code>learningRate</code>.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>inputs</strong> (numpy.ndarray):
The inputs to the network, which should match the dimensions required for
the input to the <code><a href="#NeuralNetwork.forward">forward</a></code> method of the first layer in the network.</li>
<li><strong>expectedOutputs</strong> (numpy.ndarray):
The desired values for the network's outputs. This parameter should have the same dimensions as that
which would be produced by the network's <code><a href="#NeuralNetwork.forward">forward</a></code> method.</li>
<li><strong>costFunction</strong> (CostFunction):
A cost function whose derivative is calculated with respect to the outputs of the network
(i.e. the outputs of the last layer).</li>
<li><strong>learningRate</strong> (float):
Controls the amount by which the weights and biases of each layer are changed on each step. 
Both the weights and biases of each layer are updated at the same rate.</li>
</ul>

<h6 id="notes">Notes</h6>

<p><code>CostFunction</code> is any of <code>MSE</code>, <code>BinaryCrossEntropy</code>, or <code>CategoricalCrossEntropy</code>.</p>
</div>


                            </div>
                            <div id="NeuralNetwork.train" class="classattr">
                                        <input id="NeuralNetwork.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">X</span>,</span><span class="param">	<span class="n">y</span>,</span><span class="param">	<span class="n">costFunction</span>,</span><span class="param">	<span class="n">batchSize</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>,</span><span class="param">	<span class="n">learningRate</span><span class="o">=</span><span class="mf">0.1</span>,</span><span class="param">	<span class="n">checkGradients</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">testImages</span><span class="o">=</span><span class="p">[]</span>,</span><span class="param">	<span class="n">testLabels</span><span class="o">=</span><span class="p">[]</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.train-130"><a href="#NeuralNetwork.train-130"><span class="linenos">130</span></a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">batchSize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">checkGradients</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">testImages</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">testLabels</span> <span class="o">=</span> <span class="p">[]):</span>
</span><span id="NeuralNetwork.train-131"><a href="#NeuralNetwork.train-131"><span class="linenos">131</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Trains the network using stochastic gradient descent.</span>
</span><span id="NeuralNetwork.train-132"><a href="#NeuralNetwork.train-132"><span class="linenos">132</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.train-133"><a href="#NeuralNetwork.train-133"><span class="linenos">133</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork.train-134"><a href="#NeuralNetwork.train-134"><span class="linenos">134</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork.train-135"><a href="#NeuralNetwork.train-135"><span class="linenos">135</span></a><span class="sd">        X : numpy.ndarray</span>
</span><span id="NeuralNetwork.train-136"><a href="#NeuralNetwork.train-136"><span class="linenos">136</span></a><span class="sd">            The training inputs. The first dimension equals the total number of input samples (not batches).</span>
</span><span id="NeuralNetwork.train-137"><a href="#NeuralNetwork.train-137"><span class="linenos">137</span></a><span class="sd">        Y : numpy.ndarray</span>
</span><span id="NeuralNetwork.train-138"><a href="#NeuralNetwork.train-138"><span class="linenos">138</span></a><span class="sd">            The desired training outputs. The first dimension equals the total number of input samples (not batches).</span>
</span><span id="NeuralNetwork.train-139"><a href="#NeuralNetwork.train-139"><span class="linenos">139</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="NeuralNetwork.train-140"><a href="#NeuralNetwork.train-140"><span class="linenos">140</span></a><span class="sd">            A cost function to evaluate the performance of the neural network.</span>
</span><span id="NeuralNetwork.train-141"><a href="#NeuralNetwork.train-141"><span class="linenos">141</span></a><span class="sd">        batchSize : int</span>
</span><span id="NeuralNetwork.train-142"><a href="#NeuralNetwork.train-142"><span class="linenos">142</span></a><span class="sd">            The number of input samples to include in each batch.</span>
</span><span id="NeuralNetwork.train-143"><a href="#NeuralNetwork.train-143"><span class="linenos">143</span></a><span class="sd">        epochs : int, optional</span>
</span><span id="NeuralNetwork.train-144"><a href="#NeuralNetwork.train-144"><span class="linenos">144</span></a><span class="sd">            The number of times to iterate through the full training set. Defaults to 10.</span>
</span><span id="NeuralNetwork.train-145"><a href="#NeuralNetwork.train-145"><span class="linenos">145</span></a><span class="sd">        learningRate : float, optional</span>
</span><span id="NeuralNetwork.train-146"><a href="#NeuralNetwork.train-146"><span class="linenos">146</span></a><span class="sd">            Controls the rate of learning. A small value of `learningRate` will lead to slow training,</span>
</span><span id="NeuralNetwork.train-147"><a href="#NeuralNetwork.train-147"><span class="linenos">147</span></a><span class="sd">            but a large value will cause training instability. Defaults to 0.1.</span>
</span><span id="NeuralNetwork.train-148"><a href="#NeuralNetwork.train-148"><span class="linenos">148</span></a><span class="sd">        checkGradients : bool, optional</span>
</span><span id="NeuralNetwork.train-149"><a href="#NeuralNetwork.train-149"><span class="linenos">149</span></a><span class="sd">            Whether to perform gradient checking. This was used for verifying the gradients computed through</span>
</span><span id="NeuralNetwork.train-150"><a href="#NeuralNetwork.train-150"><span class="linenos">150</span></a><span class="sd">            backpropagation were correct, and is therefore unlikely to be useful to a user. Defaults to False.</span>
</span><span id="NeuralNetwork.train-151"><a href="#NeuralNetwork.train-151"><span class="linenos">151</span></a><span class="sd">        testImages : np.ndarray, optional</span>
</span><span id="NeuralNetwork.train-152"><a href="#NeuralNetwork.train-152"><span class="linenos">152</span></a><span class="sd">            Test data to evaluate the accuracy of the model on a test set during the training process.</span>
</span><span id="NeuralNetwork.train-153"><a href="#NeuralNetwork.train-153"><span class="linenos">153</span></a><span class="sd">            Defaults to [].</span>
</span><span id="NeuralNetwork.train-154"><a href="#NeuralNetwork.train-154"><span class="linenos">154</span></a><span class="sd">        testLabels : np.ndarray, optional</span>
</span><span id="NeuralNetwork.train-155"><a href="#NeuralNetwork.train-155"><span class="linenos">155</span></a><span class="sd">            Corresponding labels for `testImages` to evaluate the accuracy of the model on a test set during</span>
</span><span id="NeuralNetwork.train-156"><a href="#NeuralNetwork.train-156"><span class="linenos">156</span></a><span class="sd">            the training process. Defaults to [].</span>
</span><span id="NeuralNetwork.train-157"><a href="#NeuralNetwork.train-157"><span class="linenos">157</span></a>
</span><span id="NeuralNetwork.train-158"><a href="#NeuralNetwork.train-158"><span class="linenos">158</span></a><span class="sd">        Notes</span>
</span><span id="NeuralNetwork.train-159"><a href="#NeuralNetwork.train-159"><span class="linenos">159</span></a><span class="sd">        -----</span>
</span><span id="NeuralNetwork.train-160"><a href="#NeuralNetwork.train-160"><span class="linenos">160</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="NeuralNetwork.train-161"><a href="#NeuralNetwork.train-161"><span class="linenos">161</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.train-162"><a href="#NeuralNetwork.train-162"><span class="linenos">162</span></a>        <span class="c1">#If the cost function is CCE, the final activation must be softmax</span>
</span><span id="NeuralNetwork.train-163"><a href="#NeuralNetwork.train-163"><span class="linenos">163</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">costFunction</span><span class="p">,</span> <span class="n">CategoricalCrossEntropy</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activationFunction</span><span class="p">,</span> <span class="n">ActivationSoftmax</span><span class="p">):</span>
</span><span id="NeuralNetwork.train-164"><a href="#NeuralNetwork.train-164"><span class="linenos">164</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The Softmax function can only be used with CCE loss&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork.train-165"><a href="#NeuralNetwork.train-165"><span class="linenos">165</span></a>        
</span><span id="NeuralNetwork.train-166"><a href="#NeuralNetwork.train-166"><span class="linenos">166</span></a>        <span class="c1">#Get number of training examples</span>
</span><span id="NeuralNetwork.train-167"><a href="#NeuralNetwork.train-167"><span class="linenos">167</span></a>        <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NeuralNetwork.train-168"><a href="#NeuralNetwork.train-168"><span class="linenos">168</span></a>
</span><span id="NeuralNetwork.train-169"><a href="#NeuralNetwork.train-169"><span class="linenos">169</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">batchSize</span><span class="p">:</span>
</span><span id="NeuralNetwork.train-170"><a href="#NeuralNetwork.train-170"><span class="linenos">170</span></a>            <span class="n">batchSize</span> <span class="o">=</span> <span class="n">m</span>
</span><span id="NeuralNetwork.train-171"><a href="#NeuralNetwork.train-171"><span class="linenos">171</span></a>
</span><span id="NeuralNetwork.train-172"><a href="#NeuralNetwork.train-172"><span class="linenos">172</span></a>        <span class="n">startIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="NeuralNetwork.train-173"><a href="#NeuralNetwork.train-173"><span class="linenos">173</span></a>
</span><span id="NeuralNetwork.train-174"><a href="#NeuralNetwork.train-174"><span class="linenos">174</span></a>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
</span><span id="NeuralNetwork.train-175"><a href="#NeuralNetwork.train-175"><span class="linenos">175</span></a>            <span class="c1">#Iterate over all batches in the training set</span>
</span><span id="NeuralNetwork.train-176"><a href="#NeuralNetwork.train-176"><span class="linenos">176</span></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span> <span class="o">//</span> <span class="n">batchSize</span><span class="p">):</span>
</span><span id="NeuralNetwork.train-177"><a href="#NeuralNetwork.train-177"><span class="linenos">177</span></a>                <span class="k">try</span><span class="p">:</span>
</span><span id="NeuralNetwork.train-178"><a href="#NeuralNetwork.train-178"><span class="linenos">178</span></a>                    <span class="c1">#Get the current batch data</span>
</span><span id="NeuralNetwork.train-179"><a href="#NeuralNetwork.train-179"><span class="linenos">179</span></a>                    <span class="n">currentBatch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">startIndex</span><span class="p">:</span><span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">]</span>
</span><span id="NeuralNetwork.train-180"><a href="#NeuralNetwork.train-180"><span class="linenos">180</span></a>                    <span class="n">currentLabels</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">startIndex</span><span class="p">:</span><span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">]</span>
</span><span id="NeuralNetwork.train-181"><a href="#NeuralNetwork.train-181"><span class="linenos">181</span></a>
</span><span id="NeuralNetwork.train-182"><a href="#NeuralNetwork.train-182"><span class="linenos">182</span></a>                    <span class="c1">#Perform a forward pass using the input data</span>
</span><span id="NeuralNetwork.train-183"><a href="#NeuralNetwork.train-183"><span class="linenos">183</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">)</span>
</span><span id="NeuralNetwork.train-184"><a href="#NeuralNetwork.train-184"><span class="linenos">184</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">updateWeights</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">,</span> <span class="n">currentLabels</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="NeuralNetwork.train-185"><a href="#NeuralNetwork.train-185"><span class="linenos">185</span></a>                    <span class="c1">#Print cost for mini batch</span>
</span><span id="NeuralNetwork.train-186"><a href="#NeuralNetwork.train-186"><span class="linenos">186</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cost: </span><span class="si">{</span><span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">currentLabels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork.train-187"><a href="#NeuralNetwork.train-187"><span class="linenos">187</span></a>
</span><span id="NeuralNetwork.train-188"><a href="#NeuralNetwork.train-188"><span class="linenos">188</span></a>                    <span class="c1">#Numerically estimate the gradients to verify backprop implementation</span>
</span><span id="NeuralNetwork.train-189"><a href="#NeuralNetwork.train-189"><span class="linenos">189</span></a>                    <span class="k">if</span> <span class="n">checkGradients</span><span class="p">:</span>
</span><span id="NeuralNetwork.train-190"><a href="#NeuralNetwork.train-190"><span class="linenos">190</span></a>                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error gradients (by backprop): </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errorGradientsSums</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork.train-191"><a href="#NeuralNetwork.train-191"><span class="linenos">191</span></a>                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Numerically estimated gradients: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">getEstimatedGradients</span><span class="p">(</span><span class="n">currentBatch</span><span class="p">,</span><span class="w"> </span><span class="n">currentLabels</span><span class="p">,</span><span class="w"> </span><span class="n">costFunction</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="NeuralNetwork.train-192"><a href="#NeuralNetwork.train-192"><span class="linenos">192</span></a>
</span><span id="NeuralNetwork.train-193"><a href="#NeuralNetwork.train-193"><span class="linenos">193</span></a>                    <span class="c1"># Evaluate accuracy on test set</span>
</span><span id="NeuralNetwork.train-194"><a href="#NeuralNetwork.train-194"><span class="linenos">194</span></a><span class="w">                    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="NeuralNetwork.train-195"><a href="#NeuralNetwork.train-195"><span class="linenos">195</span></a><span class="sd">                    if testImages != [] and testLabels != []:</span>
</span><span id="NeuralNetwork.train-196"><a href="#NeuralNetwork.train-196"><span class="linenos">196</span></a><span class="sd">                        self.forward(testImages)</span>
</span><span id="NeuralNetwork.train-197"><a href="#NeuralNetwork.train-197"><span class="linenos">197</span></a><span class="sd">                        testOutputs = np.argmax(self.getOutputs(), -1)</span>
</span><span id="NeuralNetwork.train-198"><a href="#NeuralNetwork.train-198"><span class="linenos">198</span></a><span class="sd">                        matchingCount = 0</span>
</span><span id="NeuralNetwork.train-199"><a href="#NeuralNetwork.train-199"><span class="linenos">199</span></a><span class="sd">                        for i in range(testOutputs.size):</span>
</span><span id="NeuralNetwork.train-200"><a href="#NeuralNetwork.train-200"><span class="linenos">200</span></a><span class="sd">                            if testOutputs[i] == np.argmax(testLabels[i], -1):</span>
</span><span id="NeuralNetwork.train-201"><a href="#NeuralNetwork.train-201"><span class="linenos">201</span></a><span class="sd">                                matchingCount += 1</span>
</span><span id="NeuralNetwork.train-202"><a href="#NeuralNetwork.train-202"><span class="linenos">202</span></a>
</span><span id="NeuralNetwork.train-203"><a href="#NeuralNetwork.train-203"><span class="linenos">203</span></a><span class="sd">                        print(&quot;Accuracy on test set: {}%&quot;.format((matchingCount / testOutputs.size) * 100))</span>
</span><span id="NeuralNetwork.train-204"><a href="#NeuralNetwork.train-204"><span class="linenos">204</span></a><span class="sd">                    &#39;&#39;&#39;</span>
</span><span id="NeuralNetwork.train-205"><a href="#NeuralNetwork.train-205"><span class="linenos">205</span></a>                    <span class="n">startIndex</span> <span class="o">+=</span> <span class="n">batchSize</span>
</span><span id="NeuralNetwork.train-206"><a href="#NeuralNetwork.train-206"><span class="linenos">206</span></a>
</span><span id="NeuralNetwork.train-207"><a href="#NeuralNetwork.train-207"><span class="linenos">207</span></a>                    <span class="k">if</span> <span class="n">startIndex</span> <span class="o">+</span> <span class="n">batchSize</span> <span class="o">&gt;</span> <span class="n">m</span><span class="p">:</span>
</span><span id="NeuralNetwork.train-208"><a href="#NeuralNetwork.train-208"><span class="linenos">208</span></a>                        <span class="n">startIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="NeuralNetwork.train-209"><a href="#NeuralNetwork.train-209"><span class="linenos">209</span></a>                    
</span><span id="NeuralNetwork.train-210"><a href="#NeuralNetwork.train-210"><span class="linenos">210</span></a>                <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span><span id="NeuralNetwork.train-211"><a href="#NeuralNetwork.train-211"><span class="linenos">211</span></a>                    <span class="k">break</span>
</span></pre></div>


            <div class="docstring"><p>Trains the network using stochastic gradient descent.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>X</strong> (numpy.ndarray):
The training inputs. The first dimension equals the total number of input samples (not batches).</li>
<li><strong>Y</strong> (numpy.ndarray):
The desired training outputs. The first dimension equals the total number of input samples (not batches).</li>
<li><strong>costFunction</strong> (CostFunction):
A cost function to evaluate the performance of the neural network.</li>
<li><strong>batchSize</strong> (int):
The number of input samples to include in each batch.</li>
<li><strong>epochs</strong> (int, optional):
The number of times to iterate through the full training set. Defaults to 10.</li>
<li><strong>learningRate</strong> (float, optional):
Controls the rate of learning. A small value of <code>learningRate</code> will lead to slow training,
but a large value will cause training instability. Defaults to 0.1.</li>
<li><strong>checkGradients</strong> (bool, optional):
Whether to perform gradient checking. This was used for verifying the gradients computed through
backpropagation were correct, and is therefore unlikely to be useful to a user. Defaults to False.</li>
<li><strong>testImages</strong> (np.ndarray, optional):
Test data to evaluate the accuracy of the model on a test set during the training process.
Defaults to [].</li>
<li><strong>testLabels</strong> (np.ndarray, optional):
Corresponding labels for <code>testImages</code> to evaluate the accuracy of the model on a test set during
the training process. Defaults to [].</li>
</ul>

<h6 id="notes">Notes</h6>

<p><code>CostFunction</code> is any of <code>MSE</code>, <code>BinaryCrossEntropy</code>, or <code>CategoricalCrossEntropy</code>.</p>
</div>


                            </div>
                            <div id="NeuralNetwork.getEstimatedGradients" class="classattr">
                                        <input id="NeuralNetwork.getEstimatedGradients-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">getEstimatedGradients</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputs</span>, </span><span class="param"><span class="n">yReal</span>, </span><span class="param"><span class="n">costFunction</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NeuralNetwork.getEstimatedGradients-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NeuralNetwork.getEstimatedGradients"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NeuralNetwork.getEstimatedGradients-213"><a href="#NeuralNetwork.getEstimatedGradients-213"><span class="linenos">213</span></a>    <span class="k">def</span> <span class="nf">getEstimatedGradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">,</span> <span class="n">costFunction</span><span class="p">):</span>
</span><span id="NeuralNetwork.getEstimatedGradients-214"><a href="#NeuralNetwork.getEstimatedGradients-214"><span class="linenos">214</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Numerically estimates the partial derivatives of the cost function w.r.t each weight.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-215"><a href="#NeuralNetwork.getEstimatedGradients-215"><span class="linenos">215</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.getEstimatedGradients-216"><a href="#NeuralNetwork.getEstimatedGradients-216"><span class="linenos">216</span></a><span class="sd">        This is used in gradient checking as an alternative way of computing gradients other</span>
</span><span id="NeuralNetwork.getEstimatedGradients-217"><a href="#NeuralNetwork.getEstimatedGradients-217"><span class="linenos">217</span></a><span class="sd">        than backpropagation, but is unlikely to be useful to a user.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-218"><a href="#NeuralNetwork.getEstimatedGradients-218"><span class="linenos">218</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.getEstimatedGradients-219"><a href="#NeuralNetwork.getEstimatedGradients-219"><span class="linenos">219</span></a><span class="sd">        Parameters</span>
</span><span id="NeuralNetwork.getEstimatedGradients-220"><a href="#NeuralNetwork.getEstimatedGradients-220"><span class="linenos">220</span></a><span class="sd">        ----------</span>
</span><span id="NeuralNetwork.getEstimatedGradients-221"><a href="#NeuralNetwork.getEstimatedGradients-221"><span class="linenos">221</span></a><span class="sd">        inputs : numpy.ndarray</span>
</span><span id="NeuralNetwork.getEstimatedGradients-222"><a href="#NeuralNetwork.getEstimatedGradients-222"><span class="linenos">222</span></a><span class="sd">            The inputs used for evaluating cost, and hence estimating derivatives.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-223"><a href="#NeuralNetwork.getEstimatedGradients-223"><span class="linenos">223</span></a><span class="sd">        yReal : numpy.ndarray</span>
</span><span id="NeuralNetwork.getEstimatedGradients-224"><a href="#NeuralNetwork.getEstimatedGradients-224"><span class="linenos">224</span></a><span class="sd">            The correct outputs for the given inputs, used for evaluating cost.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-225"><a href="#NeuralNetwork.getEstimatedGradients-225"><span class="linenos">225</span></a><span class="sd">        costFunction : CostFunction</span>
</span><span id="NeuralNetwork.getEstimatedGradients-226"><a href="#NeuralNetwork.getEstimatedGradients-226"><span class="linenos">226</span></a><span class="sd">            The cost function used for taking derivatives.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-227"><a href="#NeuralNetwork.getEstimatedGradients-227"><span class="linenos">227</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-228"><a href="#NeuralNetwork.getEstimatedGradients-228"><span class="linenos">228</span></a><span class="sd">        Returns</span>
</span><span id="NeuralNetwork.getEstimatedGradients-229"><a href="#NeuralNetwork.getEstimatedGradients-229"><span class="linenos">229</span></a><span class="sd">        -------</span>
</span><span id="NeuralNetwork.getEstimatedGradients-230"><a href="#NeuralNetwork.getEstimatedGradients-230"><span class="linenos">230</span></a><span class="sd">        array-like</span>
</span><span id="NeuralNetwork.getEstimatedGradients-231"><a href="#NeuralNetwork.getEstimatedGradients-231"><span class="linenos">231</span></a><span class="sd">            The estimated gradients with respect to the weights and biases of each layer.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-232"><a href="#NeuralNetwork.getEstimatedGradients-232"><span class="linenos">232</span></a><span class="sd">        </span>
</span><span id="NeuralNetwork.getEstimatedGradients-233"><a href="#NeuralNetwork.getEstimatedGradients-233"><span class="linenos">233</span></a><span class="sd">        Notes</span>
</span><span id="NeuralNetwork.getEstimatedGradients-234"><a href="#NeuralNetwork.getEstimatedGradients-234"><span class="linenos">234</span></a><span class="sd">        -----</span>
</span><span id="NeuralNetwork.getEstimatedGradients-235"><a href="#NeuralNetwork.getEstimatedGradients-235"><span class="linenos">235</span></a><span class="sd">        ``CostFunction`` is any of ``MSE``, ``BinaryCrossEntropy``, or ``CategoricalCrossEntropy``.</span>
</span><span id="NeuralNetwork.getEstimatedGradients-236"><a href="#NeuralNetwork.getEstimatedGradients-236"><span class="linenos">236</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NeuralNetwork.getEstimatedGradients-237"><a href="#NeuralNetwork.getEstimatedGradients-237"><span class="linenos">237</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-238"><a href="#NeuralNetwork.getEstimatedGradients-238"><span class="linenos">238</span></a>        <span class="c1">#Assemble a list of computed gradient matrices</span>
</span><span id="NeuralNetwork.getEstimatedGradients-239"><a href="#NeuralNetwork.getEstimatedGradients-239"><span class="linenos">239</span></a>        <span class="n">estimatedGradients</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-240"><a href="#NeuralNetwork.getEstimatedGradients-240"><span class="linenos">240</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-241"><a href="#NeuralNetwork.getEstimatedGradients-241"><span class="linenos">241</span></a>        <span class="c1"># Iterate over each bias parameter in each layer</span>
</span><span id="NeuralNetwork.getEstimatedGradients-242"><a href="#NeuralNetwork.getEstimatedGradients-242"><span class="linenos">242</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="NeuralNetwork.getEstimatedGradients-243"><a href="#NeuralNetwork.getEstimatedGradients-243"><span class="linenos">243</span></a>            <span class="n">flattenedBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-244"><a href="#NeuralNetwork.getEstimatedGradients-244"><span class="linenos">244</span></a>            <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">flattenedBiases</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-245"><a href="#NeuralNetwork.getEstimatedGradients-245"><span class="linenos">245</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flattenedBiases</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span id="NeuralNetwork.getEstimatedGradients-246"><a href="#NeuralNetwork.getEstimatedGradients-246"><span class="linenos">246</span></a>                <span class="c1">#A small value so that the gradient can be calculated between 2 points</span>
</span><span id="NeuralNetwork.getEstimatedGradients-247"><a href="#NeuralNetwork.getEstimatedGradients-247"><span class="linenos">247</span></a>                <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span><span id="NeuralNetwork.getEstimatedGradients-248"><a href="#NeuralNetwork.getEstimatedGradients-248"><span class="linenos">248</span></a>                <span class="c1">#Copy the biases matrix so that it can be replaced once gradient checking completes</span>
</span><span id="NeuralNetwork.getEstimatedGradients-249"><a href="#NeuralNetwork.getEstimatedGradients-249"><span class="linenos">249</span></a>                <span class="n">biasesCopy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="NeuralNetwork.getEstimatedGradients-250"><a href="#NeuralNetwork.getEstimatedGradients-250"><span class="linenos">250</span></a>                <span class="n">flattenedBiases</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork.getEstimatedGradients-251"><a href="#NeuralNetwork.getEstimatedGradients-251"><span class="linenos">251</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-252"><a href="#NeuralNetwork.getEstimatedGradients-252"><span class="linenos">252</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-253"><a href="#NeuralNetwork.getEstimatedGradients-253"><span class="linenos">253</span></a>                <span class="n">gradPlus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-254"><a href="#NeuralNetwork.getEstimatedGradients-254"><span class="linenos">254</span></a>                <span class="n">flattenedBiases</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork.getEstimatedGradients-255"><a href="#NeuralNetwork.getEstimatedGradients-255"><span class="linenos">255</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-256"><a href="#NeuralNetwork.getEstimatedGradients-256"><span class="linenos">256</span></a>                <span class="n">gradMinus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-257"><a href="#NeuralNetwork.getEstimatedGradients-257"><span class="linenos">257</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-258"><a href="#NeuralNetwork.getEstimatedGradients-258"><span class="linenos">258</span></a>                <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradPlus</span> <span class="o">-</span> <span class="n">gradMinus</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-259"><a href="#NeuralNetwork.getEstimatedGradients-259"><span class="linenos">259</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-260"><a href="#NeuralNetwork.getEstimatedGradients-260"><span class="linenos">260</span></a>                <span class="c1">#Re-instate original layer biases</span>
</span><span id="NeuralNetwork.getEstimatedGradients-261"><a href="#NeuralNetwork.getEstimatedGradients-261"><span class="linenos">261</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">biasesCopy</span>
</span><span id="NeuralNetwork.getEstimatedGradients-262"><a href="#NeuralNetwork.getEstimatedGradients-262"><span class="linenos">262</span></a>                <span class="n">flattenedBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-263"><a href="#NeuralNetwork.getEstimatedGradients-263"><span class="linenos">263</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-264"><a href="#NeuralNetwork.getEstimatedGradients-264"><span class="linenos">264</span></a>        <span class="c1">#Iterate over each weight parameter in each layer</span>
</span><span id="NeuralNetwork.getEstimatedGradients-265"><a href="#NeuralNetwork.getEstimatedGradients-265"><span class="linenos">265</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
</span><span id="NeuralNetwork.getEstimatedGradients-266"><a href="#NeuralNetwork.getEstimatedGradients-266"><span class="linenos">266</span></a>            <span class="n">flattenedWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-267"><a href="#NeuralNetwork.getEstimatedGradients-267"><span class="linenos">267</span></a>            <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">flattenedWeights</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-268"><a href="#NeuralNetwork.getEstimatedGradients-268"><span class="linenos">268</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flattenedWeights</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span id="NeuralNetwork.getEstimatedGradients-269"><a href="#NeuralNetwork.getEstimatedGradients-269"><span class="linenos">269</span></a>                <span class="c1">#A small value so that the gradient can be calculated between 2 points</span>
</span><span id="NeuralNetwork.getEstimatedGradients-270"><a href="#NeuralNetwork.getEstimatedGradients-270"><span class="linenos">270</span></a>                <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span><span id="NeuralNetwork.getEstimatedGradients-271"><a href="#NeuralNetwork.getEstimatedGradients-271"><span class="linenos">271</span></a>                <span class="c1">#Copy the weights matrix so that it can be replaced once gradient checking completes</span>
</span><span id="NeuralNetwork.getEstimatedGradients-272"><a href="#NeuralNetwork.getEstimatedGradients-272"><span class="linenos">272</span></a>                <span class="n">weightsCopy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="NeuralNetwork.getEstimatedGradients-273"><a href="#NeuralNetwork.getEstimatedGradients-273"><span class="linenos">273</span></a>                <span class="n">flattenedWeights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork.getEstimatedGradients-274"><a href="#NeuralNetwork.getEstimatedGradients-274"><span class="linenos">274</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-275"><a href="#NeuralNetwork.getEstimatedGradients-275"><span class="linenos">275</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-276"><a href="#NeuralNetwork.getEstimatedGradients-276"><span class="linenos">276</span></a>                <span class="n">gradPlus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-277"><a href="#NeuralNetwork.getEstimatedGradients-277"><span class="linenos">277</span></a>                <span class="n">flattenedWeights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span>
</span><span id="NeuralNetwork.getEstimatedGradients-278"><a href="#NeuralNetwork.getEstimatedGradients-278"><span class="linenos">278</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-279"><a href="#NeuralNetwork.getEstimatedGradients-279"><span class="linenos">279</span></a>                <span class="n">gradMinus</span> <span class="o">=</span> <span class="n">costFunction</span><span class="o">.</span><span class="n">getCost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">yReal</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-280"><a href="#NeuralNetwork.getEstimatedGradients-280"><span class="linenos">280</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-281"><a href="#NeuralNetwork.getEstimatedGradients-281"><span class="linenos">281</span></a>                <span class="n">estimatedGradients</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradPlus</span> <span class="o">-</span> <span class="n">gradMinus</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">EPSILON</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-282"><a href="#NeuralNetwork.getEstimatedGradients-282"><span class="linenos">282</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-283"><a href="#NeuralNetwork.getEstimatedGradients-283"><span class="linenos">283</span></a>                <span class="c1">#Re-instate original layer weights</span>
</span><span id="NeuralNetwork.getEstimatedGradients-284"><a href="#NeuralNetwork.getEstimatedGradients-284"><span class="linenos">284</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weightsCopy</span>
</span><span id="NeuralNetwork.getEstimatedGradients-285"><a href="#NeuralNetwork.getEstimatedGradients-285"><span class="linenos">285</span></a>                <span class="n">flattenedWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="NeuralNetwork.getEstimatedGradients-286"><a href="#NeuralNetwork.getEstimatedGradients-286"><span class="linenos">286</span></a>
</span><span id="NeuralNetwork.getEstimatedGradients-287"><a href="#NeuralNetwork.getEstimatedGradients-287"><span class="linenos">287</span></a>        <span class="k">return</span> <span class="n">estimatedGradients</span>
</span></pre></div>


            <div class="docstring"><p>Numerically estimates the partial derivatives of the cost function w.r.t each weight.</p>

<p>This is used in gradient checking as an alternative way of computing gradients other
than backpropagation, but is unlikely to be useful to a user.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>inputs</strong> (numpy.ndarray):
The inputs used for evaluating cost, and hence estimating derivatives.</li>
<li><strong>yReal</strong> (numpy.ndarray):
The correct outputs for the given inputs, used for evaluating cost.</li>
<li><strong>costFunction</strong> (CostFunction):
The cost function used for taking derivatives.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>array-like</strong>: The estimated gradients with respect to the weights and biases of each layer.</li>
</ul>

<h6 id="notes">Notes</h6>

<p><code>CostFunction</code> is any of <code>MSE</code>, <code>BinaryCrossEntropy</code>, or <code>CategoricalCrossEntropy</code>.</p>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>